{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moo-337/differential-machine-learning/blob/Auto-Call/DifferentialMLTF2_AutoCall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH28r65zmri7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/differential-machine-learning/notebooks/blob/master/DifferentialMLTF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icd9ZKB6mrjB"
      },
      "source": [
        "Working paper: https://arxiv.org/abs/2005.02347 \n",
        "GitHub: https://github.com/differential-machine-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL7p_WcDmrjC"
      },
      "source": [
        "# Differential Deep Learning\n",
        "\n",
        "---\n",
        "Updated for TensorFlow 2\n",
        "---\n",
        "\n",
        "---\n",
        "Antoine Savine, January 2021\n",
        "---\n",
        "\n",
        "This notebook implements the novel ideas of *twin networks* and *differential training* from the working paper [Differential Machine Learning](https://arxiv.org/abs/2005.02347) by Brian Huge and [Antoine Savine](https://antoinesavine.com) (2020), and applies them in a few simple contexts, including the reproduction of some results from the paper. In the article, we presented the ideas and discussed the algorithms, but skipped important implementation details best covered in a notebook with code. Those details are dicussed here.    \n",
        "\n",
        "Although this is a *demonstration* notebook, the algorithms are implemented in self contained, modular and extensible blocks of code. They may be easily extracted, extended and integrated into production systems. Some advanced extensions particularly important in a production context are discussed in the [appendices](https://github.com/differential-machine-learning/appendices/).\n",
        "\n",
        "The code is based on TensorFlow 1.x. You may run the notebook in a local TensorFlow 1.x enviroment *with GPU support*, also with common libraries like numpy and matplotlib. On Google Colab, the correct version of TensorFlow is automatically selected, and all the necessary libraries are preinstalled. All you need is *enable GPU support*: menu runtime/change runtime type and select hardware accelerator GPU.\n",
        "\n",
        "We do not further discuss the algorithms covered in the article or fundamentals of TensorFlow (see classic resources like [Geron's textbook](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291), first edition for TensorFlow 1.x). For modern deep learning and best practices, we refer e.g. to [Goodfellow and al's Deep Learning Book](https://www.deeplearningbook.org/) or [Andrew Ng's deep learning lectures](https://www.coursera.org/specializations/deep-learning). \n",
        "\n",
        "This notebook was ported on TensorFlow 2 in January 2021. The original TensorFlow 1 notebook will no longer be updated. For now, the notebook runs TensorFlow 1 code in a TensorFlow 2 environment. An upgrade to idiomatic TensorFlow 2 code will be available shortly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQWN6m4tmrjD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RvNyZtcmrjE"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    %matplotlib notebook\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# import and test\n",
        "import tensorflow as tf2\n",
        "print(\"TF version =\", tf2.__version__)\n",
        "\n",
        "# we want TF 2.x\n",
        "assert tf2.__version__ >= \"2.0\"\n",
        "\n",
        "# disable eager execution etc\n",
        "tf = tf2.compat.v1\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "# disable annoying warnings\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# make sure we have GPU support\n",
        "print(\"GPU support = \", tf.test.is_gpu_available())\n",
        "\n",
        "# import other useful libs\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "import copy\n",
        "\n",
        "# representation of real numbers in TF, change here for 32/64 bits\n",
        "real_type = tf.float32\n",
        "# real_type = tf.float64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_kG4MMCmrjH"
      },
      "source": [
        "# Part I : Core Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsSye-jvmrjI"
      },
      "source": [
        "## Feedforward neural network in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wHb07vTmrjI"
      },
      "source": [
        "This function builds a classic feedforward neural network in TensorFlow, implementing the set of equations $(3)$ from the article. This is classic deep learning code and a direct translation of equations $(3)$ in Python. We tried to keep notations consistent with the paper.\n",
        "\n",
        "The weights are initialized with TensorFlow's  [variance_scaling_initializer](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/initializers/variance_scaling), implementing the particularly effective [Xavier/Glorot](https://www.deeplearning.ai/ai-notes/initialization/)  initialization strategy, a best practice in modern deep learning. A correct initialization is a key ingredient in an effective practical implementation of deep learning. \n",
        "\n",
        "Since the network is initialized randomly, we also implement seed in the interest of reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBxxCK23mrjJ"
      },
      "outputs": [],
      "source": [
        "def vanilla_net(\n",
        "    input_dim,      # dimension of inputs, e.g. 10\n",
        "    hidden_units,   # units in hidden layers, assumed constant, e.g. 20\n",
        "    hidden_layers,  # number of hidden layers, e.g. 4\n",
        "    seed):          # seed for initialization or None for random\n",
        "    \n",
        "    # set seed\n",
        "    tf.set_random_seed(seed)\n",
        "    \n",
        "    # input layer\n",
        "    xs = tf.placeholder(shape=[None, input_dim], dtype=real_type)\n",
        "    \n",
        "    # connection weights and biases of hidden layers\n",
        "    ws = [None]\n",
        "    bs = [None]\n",
        "    # layer 0 (input) has no parameters\n",
        "    \n",
        "    # layer 0 = input layer\n",
        "    zs = [xs] # eq.3, l=0\n",
        "    \n",
        "    # first hidden layer (index 1)\n",
        "    # weight matrix\n",
        "    ws.append(tf.get_variable(\"w1\", [input_dim, hidden_units], \\\n",
        "        initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "    # bias vector\n",
        "    bs.append(tf.get_variable(\"b1\", [hidden_units], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # graph\n",
        "    zs.append(zs[0] @ ws[1] + bs[1]) # eq. 3, l=1\n",
        "    \n",
        "    # second hidden layer (index 2) to last (index hidden_layers)\n",
        "    for l in range(1, hidden_layers): \n",
        "        ws.append(tf.get_variable(\"w%d\"%(l+1), [hidden_units, hidden_units], \\\n",
        "            initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "        bs.append(tf.get_variable(\"b%d\"%(l+1), [hidden_units], \\\n",
        "            initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "        zs.append(tf.nn.softplus(zs[l]) @ ws[l+1] + bs[l+1]) # eq. 3, l=2..L-1\n",
        "\n",
        "    # output layer (index hidden_layers+1)\n",
        "    ws.append(tf.get_variable(\"w\"+str(hidden_layers+1), [hidden_units, 1], \\\n",
        "            initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "    bs.append(tf.get_variable(\"b\"+str(hidden_layers+1), [1], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # eq. 3, l=L\n",
        "    zs.append(tf.nn.softplus(zs[hidden_layers]) @ ws[hidden_layers+1] + bs[hidden_layers+1]) \n",
        "    \n",
        "    # result = output layer\n",
        "    ys = zs[hidden_layers+1]\n",
        "    \n",
        "    # return input layer, (parameters = weight matrices and bias vectors), \n",
        "    # [all layers] and output layer\n",
        "    return xs, (ws, bs), zs, ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmyLZWn2mrjK"
      },
      "source": [
        "## Explicit backpropagation and twin network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8JUvb-6mrjK"
      },
      "source": [
        "These functions construct the twin network by explicit backpropagation (eq. $(4)$ in the article). The twin network simultaneously and efficiently predicts values and their differentials to inputs, allowing to train on datasets augmented with differential labels (i.e. gradients of labels to inputs, as explained in the working paper). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkL1n1gxmrjL"
      },
      "outputs": [],
      "source": [
        "# compute d_output/d_inputs by (explicit) backprop in vanilla net\n",
        "def backprop(\n",
        "    weights_and_biases, # 2nd output from vanilla_net() \n",
        "    zs):                # 3rd output from vanilla_net()\n",
        "    \n",
        "    ws, bs = weights_and_biases\n",
        "    L = len(zs) - 1\n",
        "    \n",
        "    # backpropagation, eq. 4, l=L..1\n",
        "    zbar = tf.ones_like(zs[L]) # zbar_L = 1\n",
        "    for l in range(L-1, 0, -1):\n",
        "        zbar = (zbar @ tf.transpose(ws[l+1])) * tf.nn.sigmoid(zs[l]) # eq. 4\n",
        "    # for l=0\n",
        "    zbar = zbar @ tf.transpose(ws[1]) # eq. 4\n",
        "    \n",
        "    xbar = zbar # xbar = zbar_0\n",
        "    \n",
        "    # dz[L] / dx\n",
        "    return xbar    \n",
        "\n",
        "# combined graph for valuation and differentiation\n",
        "def twin_net(input_dim, hidden_units, hidden_layers, seed):\n",
        "    \n",
        "    # first, build the feedforward net\n",
        "    xs, (ws, bs), zs, ys = vanilla_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # then, build its differentiation by backprop\n",
        "    xbar = backprop((ws, bs), zs)\n",
        "    \n",
        "    # return input x, output y and differentials d_y/d_z\n",
        "    return xs, ys, xbar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZqSQHucmrjL"
      },
      "source": [
        "## Vanilla training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkx0LS6mrjM"
      },
      "source": [
        "These are classic training loops for the feedforward neural network. As customary in modern deep learning, the training set is traversed in mini-batches, where the cost function is minimized with the best practice [ADAM](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdamOptimizer) algorithm. Andrew Ng's deeplearning.ai web site has an [intuitive presentation of various optimization algorithms in deep learning](https://www.deeplearning.ai/ai-notes/optimization/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TykxHUJymrjM"
      },
      "outputs": [],
      "source": [
        "def vanilla_training_graph(input_dim, hidden_units, hidden_layers, seed):\n",
        "    \n",
        "    # net\n",
        "    inputs, weights_and_biases, layers, predictions = \\\n",
        "        vanilla_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # backprop even though we are not USING differentials for training\n",
        "    # we still need them to predict derivatives dy_dx \n",
        "    derivs_predictions = backprop(weights_and_biases, layers)\n",
        "    \n",
        "    # placeholder for labels\n",
        "    labels = tf.placeholder(shape=[None, 1], dtype=real_type)\n",
        "    \n",
        "    # loss \n",
        "    loss = tf.losses.mean_squared_error(labels, predictions)\n",
        "    \n",
        "    # optimizer\n",
        "    learning_rate = tf.placeholder(real_type)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    \n",
        "    # return all necessary \n",
        "    return inputs, labels, predictions, derivs_predictions, learning_rate, loss, optimizer.minimize(loss)\n",
        "\n",
        "# training loop for one epoch\n",
        "def vanilla_train_one_epoch(# training graph from vanilla_training_graph()\n",
        "                            inputs, labels, lr_placeholder, minimizer,   \n",
        "                            # training set \n",
        "                            x_train, y_train,                           \n",
        "                            # params, left to client code\n",
        "                            learning_rate, batch_size, session):        \n",
        "    \n",
        "    m, n = x_train.shape\n",
        "    \n",
        "    # minimization loop over mini-batches\n",
        "    first = 0\n",
        "    last = min(batch_size, m)\n",
        "    while first < m:\n",
        "        session.run(minimizer, feed_dict = {\n",
        "            inputs: x_train[first:last], \n",
        "            labels: y_train[first:last],\n",
        "            lr_placeholder: learning_rate\n",
        "        })\n",
        "        first = last\n",
        "        last = min(first + batch_size, m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK09hmjcmrjM"
      },
      "source": [
        "## Differential training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb_2921xmrjM"
      },
      "source": [
        "The apparently similar *differential* training loop implements the main idea from the paper, to train twin networks on datasets augmented with differentials of labels to inputs, by minimization of a combined cost function reflecting errors in both predicted values *and predicted derivatives*:\n",
        "\n",
        "$$\n",
        "    c = \\alpha c_{\\text{val}} + \\beta c_{\\text{diff}}\n",
        "$$\n",
        "\n",
        "where $c_{\\text{val}}$ is the classic mean square error (MSE) of predictions to labels and $c_{\\text{diff}}$ is the cost of wrong derivatives:\n",
        "\n",
        "$$\n",
        "    c_{\\text{diff}} = \\frac{\\sum_{\\text{inputs j}} \\lambda_j^2 {mse}_j}{\\text{num inputs}}\n",
        "$$\n",
        "\n",
        "where ${mse}_j$ is the mean squared error of derivatives to input $j$ and the weights $\\lambda_j$ normalize derivatives so all the components of the cost have similar magnitudes, as explained in the article. We provide a basic normalization formula for the weights a few cells below, and a complete review of data preparation and weighting in [this appendix](https://github.com/differential-machine-learning/appendices/blob/master/App2-Preprocessing.pdf).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGLZK34YmrjN"
      },
      "outputs": [],
      "source": [
        "def diff_training_graph(\n",
        "    # same as vanilla\n",
        "    input_dim, \n",
        "    hidden_units, \n",
        "    hidden_layers, \n",
        "    seed, \n",
        "    # balance relative weight of values and differentials \n",
        "    # loss = alpha * MSE(values) + beta * MSE(greeks, lambda_j) \n",
        "    # see online appendix\n",
        "    alpha, \n",
        "    beta,\n",
        "    lambda_j):\n",
        "    \n",
        "    # net, now a twin\n",
        "    inputs, predictions, derivs_predictions = twin_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # placeholder for labels, now also derivs labels\n",
        "    labels = tf.placeholder(shape=[None, 1], dtype=real_type)\n",
        "    derivs_labels = tf.placeholder(shape=[None, derivs_predictions.shape[1]], dtype=real_type)\n",
        "    \n",
        "    # loss, now combined values + derivatives\n",
        "    loss = alpha * tf.losses.mean_squared_error(labels, predictions) \\\n",
        "    + beta * tf. losses.mean_squared_error(derivs_labels * lambda_j, derivs_predictions * lambda_j)\n",
        "    \n",
        "    # optimizer, as vanilla\n",
        "    learning_rate = tf.placeholder(real_type)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    \n",
        "    # return all necessary tensors, including derivatives\n",
        "    # predictions and labels\n",
        "    return inputs, labels, derivs_labels, predictions, derivs_predictions, \\\n",
        "            learning_rate, loss, optimizer.minimize(loss)\n",
        "\n",
        "def diff_train_one_epoch(inputs, labels, derivs_labels, \n",
        "                         # graph\n",
        "                         lr_placeholder, minimizer,             \n",
        "                         # training set, extended\n",
        "                         x_train, y_train, dydx_train,          \n",
        "                         # params\n",
        "                         learning_rate, batch_size, session):   \n",
        "    \n",
        "    m, n = x_train.shape\n",
        "    \n",
        "    # minimization loop, now with Greeks\n",
        "    first = 0\n",
        "    last = min(batch_size, m)\n",
        "    while first < m:\n",
        "        session.run(minimizer, feed_dict = {\n",
        "            inputs: x_train[first:last], \n",
        "            labels: y_train[first:last],\n",
        "            derivs_labels: dydx_train[first:last],\n",
        "            lr_placeholder: learning_rate\n",
        "        })\n",
        "        first = last\n",
        "        last = min(first + batch_size, m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEwj2FUfmrjN"
      },
      "source": [
        "## Combined outer training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q34b2CtImrjO"
      },
      "source": [
        "The outer training loop optimizes the weights of neural approximators for a number of epochs (complete sweeps of the training set). We applied the recent [one-cycle learning rate schedule](https://arxiv.org/abs/1803.09820) of Leslie Smith and found that it considerably accelerates and stabilizes the training of neural networks. 100 epochs is more than sufficient in most practical cases. A convergence and/or cross-validation test may be included for early stopping. Typical training takes around a second on a decent GPU (longer on Colab's shared GPUs). The approximator class, defined next, holds all the necessary data and parameters, along with the TensorFlow graph and computing session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1COeYOwOmrjO"
      },
      "outputs": [],
      "source": [
        "def train(description,\n",
        "          # neural approximator\n",
        "          approximator,              \n",
        "          # training params\n",
        "          reinit=True, \n",
        "          epochs=100, \n",
        "          # one-cycle learning rate schedule\n",
        "          learning_rate_schedule=[    (0.0, 1.0e-8), \\\n",
        "                                      (0.2, 0.1),    \\\n",
        "                                      (0.6, 0.01),   \\\n",
        "                                      (0.9, 1.0e-6), \\\n",
        "                                      (1.0, 1.0e-8)  ], \n",
        "          batches_per_epoch=16,\n",
        "          min_batch_size=256,\n",
        "          # callback function and when to call it\n",
        "          callback=None,           # arbitrary callable\n",
        "          callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "    # batching\n",
        "    batch_size = max(min_batch_size, approximator.m // batches_per_epoch)\n",
        "    \n",
        "    # one-cycle learning rate sechedule\n",
        "    lr_schedule_epochs, lr_schedule_rates = zip(*learning_rate_schedule)\n",
        "            \n",
        "    # reset\n",
        "    if reinit:\n",
        "        approximator.session.run(approximator.initializer)\n",
        "    \n",
        "    # callback on epoch 0, if requested\n",
        "    if callback and 0 in callback_epochs:\n",
        "        callback(approximator, 0)\n",
        "        \n",
        "    # loop on epochs, with progress bar (tqdm)\n",
        "    for epoch in tqdm_notebook(range(epochs), desc=description):\n",
        "        \n",
        "        # interpolate learning rate in cycle\n",
        "        learning_rate = np.interp(epoch / epochs, lr_schedule_epochs, lr_schedule_rates)\n",
        "        \n",
        "        # train one epoch\n",
        "        \n",
        "        if not approximator.differential:\n",
        "        \n",
        "            vanilla_train_one_epoch(\n",
        "                approximator.inputs, \n",
        "                approximator.labels, \n",
        "                approximator.learning_rate, \n",
        "                approximator.minimizer, \n",
        "                approximator.x, \n",
        "                approximator.y, \n",
        "                learning_rate, \n",
        "                batch_size, \n",
        "                approximator.session)\n",
        "        \n",
        "        else:\n",
        "        \n",
        "            diff_train_one_epoch(\n",
        "                approximator.inputs, \n",
        "                approximator.labels, \n",
        "                approximator.derivs_labels,\n",
        "                approximator.learning_rate, \n",
        "                approximator.minimizer, \n",
        "                approximator.x, \n",
        "                approximator.y, \n",
        "                approximator.dy_dx,\n",
        "                learning_rate, \n",
        "                batch_size, \n",
        "                approximator.session)\n",
        "        \n",
        "        # callback, if requested\n",
        "        if callback and epoch in callback_epochs:\n",
        "            callback(approximator, epoch)\n",
        "\n",
        "    # final callback, if requested\n",
        "    if callback and epochs in callback_epochs:\n",
        "        callback(approximator, epochs)        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnmDGIYvmrjP"
      },
      "source": [
        "## Data normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGu-6xGlmrjP"
      },
      "source": [
        "The practical performance of neural networks strongly depends on implementation details, like weight initialization and optimization. Another crucial practicality is the *normalization of training data*. We refer to deep learning texbooks for a discussion of the importance of normalization. One reason is that we need hyperparameters like the learning rate schedule to remain constant over datasets. If notional was to be increased by factor 1M all things equal, gradients would be multiplied by 1M too and learning rates would have to be divided by 1M to keep things similar. Normalizing data avoids manual tinkering of hyperparameters for different datasets. All the examples in the paper: the Gaussian basket, the autocallable trade and the netting set, were all approximated with the exact same hyperparameters. This is only possible with normalized datasets. \n",
        "\n",
        "We implement below a *basic* normalization strategy, where the training inputs and labels are normalized by mean and standard deviation, with differentials normalized accordingly. The differential weights in the cost function $\\lambda_j$ divide costs by the norm of the normalized differentials, keeping similar the magnitude of all the components of the cost.\n",
        "\n",
        "We can do a lot better, especially with differential labels. We could perform successive changes of basis, combined with filtering strategies, to feed neural networks with data orthonormal in terms of inputs *and* in terms of differentials, highlighting the most significant risk factors and filtering out the irrelevant ones. This is covered in detail in the [this appendix](https://github.com/differential-machine-learning/appendices/blob/master/App2-Preprocessing.pdf), where we discuss novel, and extremely effective data preparation algorithms, exploiting information contained in differential labels to considerably alleviate the load of training neural networks and subsequently, improve training performance and reduce sensitivity to hyperparameters. \n",
        "\n",
        "Although the basic normalization implemented here is sufficient for textbook examples, a reliable implementation in production for arbitrary schedules of cash-flows, resilient to very high dimensionality, necessitatesthe more complete data preparation.\n",
        "\n",
        "Note that the prediction of values and derivatives must be adjusted accordingly: prediction inputs must be normalized, and resulting predictions must be 'un-normalized'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5Y8H7bGmrjQ"
      },
      "outputs": [],
      "source": [
        "# basic data preparation\n",
        "epsilon = 1.0e-08\n",
        "def normalize_data(x_raw, y_raw, dydx_raw=None, crop=None):\n",
        "    \n",
        "    # crop dataset\n",
        "    m = crop if crop is not None else x_raw.shape[0]\n",
        "    x_cropped = x_raw[:m]\n",
        "    y_cropped = y_raw[:m]\n",
        "    dycropped_dxcropped = dydx_raw[:m] if dydx_raw is not None else None\n",
        "    \n",
        "    # normalize dataset\n",
        "    x_mean = x_cropped.mean(axis=0)\n",
        "    x_std = x_cropped.std(axis=0) + epsilon\n",
        "    x = (x_cropped- x_mean) / x_std\n",
        "    y_mean = y_cropped.mean(axis=0)\n",
        "    y_std = y_cropped.std(axis=0) + epsilon\n",
        "    y = (y_cropped-y_mean) / y_std\n",
        "    \n",
        "    # normalize derivatives too\n",
        "    if dycropped_dxcropped is not None:\n",
        "        dy_dx = dycropped_dxcropped / y_std * x_std \n",
        "        # weights of derivatives in cost function = (quad) mean size\n",
        "        lambda_j = 1.0 / np.sqrt((dy_dx ** 2).mean(axis=0)).reshape(1, -1)\n",
        "    else:\n",
        "        dy_dx = None\n",
        "        lambda_j = None\n",
        "    \n",
        "    return x_mean, x_std, x, y_mean, y_std, y, dy_dx, lambda_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scvIqupAmrjQ"
      },
      "source": [
        "## Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMzNB3n8mrjQ"
      },
      "source": [
        "For convenience, we put it all together in a *Neural_Approximator* class. Most of the code should be self explanatory. \n",
        "\n",
        "Note that we compute the coefficients $\\alpha$ and $\\beta$ for balancing cost between values and derivatives in a straightforward manner:\n",
        "\n",
        "$$\n",
        "    \\alpha = \\frac{1}{1 + \\lambda n} \\text{ and }  \\beta = \\frac{\\lambda n}{1 + \\lambda n}\n",
        "$$\n",
        "\n",
        "where $n$ is the number of inputs, so an error on a derivative has a weight similar to a value error, and $\\lambda$ is a hyperparameter without significant effect, as explained in the paper, and left to 1, safe for debugging.\n",
        "\n",
        "We implement a simple, feedforward architecture with 4 hidden layers of 20 units, throughout our tests. We kept the same architecture in production, with the addition of a *wide and deep* layer, inspired by [Google](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) and discussed in the [this appendix](https://github.com/differential-machine-learning/appendices/blob/master/App4-UnsupervisedTraining.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXp2F-bImrjR"
      },
      "outputs": [],
      "source": [
        "class Neural_Approximator():\n",
        "    \n",
        "    def __init__(self, x_raw, y_raw, \n",
        "                 dydx_raw=None):      # derivatives labels, \n",
        "       \n",
        "        self.x_raw = x_raw\n",
        "        self.y_raw = y_raw\n",
        "        self.dydx_raw = dydx_raw\n",
        "        \n",
        "        # tensorflow logic\n",
        "        self.graph = None\n",
        "        self.session = None\n",
        "                        \n",
        "    def __del__(self):\n",
        "        if self.session is not None:\n",
        "            self.session.close()\n",
        "        \n",
        "    def build_graph(self,\n",
        "                differential,       # differential or not           \n",
        "                lam,                # balance cost between values and derivs  \n",
        "                hidden_units, \n",
        "                hidden_layers, \n",
        "                weight_seed):\n",
        "        \n",
        "        # first, deal with tensorflow logic\n",
        "        if self.session is not None:\n",
        "            self.session.close()\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "        \n",
        "        with self.graph.as_default():\n",
        "        \n",
        "            # build the graph, either vanilla or differential\n",
        "            self.differential = differential\n",
        "            \n",
        "            if not differential:\n",
        "            # vanilla \n",
        "                \n",
        "                self.inputs, \\\n",
        "                self.labels, \\\n",
        "                self.predictions, \\\n",
        "                self.derivs_predictions, \\\n",
        "                self.learning_rate, \\\n",
        "                self.loss, \\\n",
        "                self.minimizer \\\n",
        "                = vanilla_training_graph(self.n, hidden_units, hidden_layers, weight_seed)\n",
        "                    \n",
        "            else:\n",
        "            # differential\n",
        "            \n",
        "                if self.dy_dx is None:\n",
        "                    raise Exception(\"No differential labels for differential training graph\")\n",
        "            \n",
        "                self.alpha = 1.0 / (1.0 + lam * self.n)\n",
        "                self.beta = 1.0 - self.alpha\n",
        "                \n",
        "                self.inputs, \\\n",
        "                self.labels, \\\n",
        "                self.derivs_labels, \\\n",
        "                self.predictions, \\\n",
        "                self.derivs_predictions, \\\n",
        "                self.learning_rate, \\\n",
        "                self.loss, \\\n",
        "                self.minimizer = diff_training_graph(self.n, hidden_units, \\\n",
        "                                                     hidden_layers, weight_seed, \\\n",
        "                                                     self.alpha, self.beta, self.lambda_j)\n",
        "        \n",
        "            # global initializer\n",
        "            self.initializer = tf.global_variables_initializer()\n",
        "            \n",
        "        # done\n",
        "        self.graph.finalize()\n",
        "        self.session = tf.Session(graph=self.graph)\n",
        "                        \n",
        "    # prepare for training with m examples, standard or differential\n",
        "    def prepare(self, \n",
        "                m, \n",
        "                differential,\n",
        "                lam=1,              # balance cost between values and derivs  \n",
        "                # standard architecture\n",
        "                hidden_units=20, \n",
        "                hidden_layers=4, \n",
        "                weight_seed=None):\n",
        "\n",
        "        # prepare dataset\n",
        "        self.x_mean, self.x_std, self.x, self.y_mean, self.y_std, self.y, self.dy_dx, self.lambda_j = \\\n",
        "            normalize_data(self.x_raw, self.y_raw, self.dydx_raw, m)\n",
        "        \n",
        "        # build graph        \n",
        "        self.m, self.n = self.x.shape        \n",
        "        self.build_graph(differential, lam, hidden_units, hidden_layers, weight_seed)\n",
        "        \n",
        "    def train(self,            \n",
        "              description=\"training\",\n",
        "              # training params\n",
        "              reinit=True, \n",
        "              epochs=100, \n",
        "              # one-cycle learning rate schedule\n",
        "              learning_rate_schedule=[\n",
        "                  (0.0, 1.0e-8), \n",
        "                  (0.2, 0.1), \n",
        "                  (0.6, 0.01), \n",
        "                  (0.9, 1.0e-6), \n",
        "                  (1.0, 1.0e-8)], \n",
        "              batches_per_epoch=16,\n",
        "              min_batch_size=256,\n",
        "              # callback and when to call it\n",
        "              # we don't use callbacks, but this is very useful, e.g. for debugging\n",
        "              callback=None,           # arbitrary callable\n",
        "              callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "        train(description, \n",
        "              self, \n",
        "              reinit, \n",
        "              epochs, \n",
        "              learning_rate_schedule, \n",
        "              batches_per_epoch, \n",
        "              min_batch_size,\n",
        "              callback, \n",
        "              callback_epochs)\n",
        "     \n",
        "    def predict_values(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std \n",
        "        # predict scaled\n",
        "        y_scaled = self.session.run(self.predictions, feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        return y\n",
        "\n",
        "    def predict_values_and_derivs(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std\n",
        "        # predict scaled\n",
        "        y_scaled, dyscaled_dxscaled = self.session.run(\n",
        "            [self.predictions, self.derivs_predictions], \n",
        "            feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        dydx = self.y_std / self.x_std * dyscaled_dxscaled\n",
        "        return y, dydx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_EKef5nmrjR"
      },
      "source": [
        "# Part II : Learning Pricing and Risk Functions from LSM samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AyFFxfxmrjS"
      },
      "source": [
        "As explained in the article, to learn pricing approximations from ground truth labels is not viable for financial applications like regulations or scenario-based risk reports, because the ground truth prices of complex transactions and trading books must be computed with numerical methods like nested Monte-Carlo simulations, where the computation load makes it impossible to simulate training sets in realistic time.\n",
        "\n",
        "Instead, we **learn from samples** *a la* Lonstaff-Schwartz (or **LSM**, from Valuing American Options by Simulation: A Simple Least-Squares Approach, The Review of Financial Studies, 2001). Our inputs are simulated Markov states at some time $T_1$ (called *exposure date* in the context of regulations), labels are *payoffs* (sum of discounted cash-flows paid after $T_1$) picked on the same Monte-Carlo path, and differential labels are **pathwise derivatives** usually computed with automatic adjoint differentiation (AAD, see [textbook](https://amazon.com/Modern-Computational-Finance-Parallel-Simulations-dp-1119539455/dp/1119539455)) (although in the simple cases of Black & Scholes and Bachelier, we easily compute them explicitly).\n",
        "\n",
        "It follows that training sets are produced quickly, for a computation cost similar to *one* Monte-Carlo price, but we now expect the machine to learn pricing and risk functions without having ever seen a price or a delta, only samples. That training still converges to the correct pricing and risk function may be seen somewhat magical. Fortunately, Longstaff & Schwartz' idea of training on samples is backed by solid mathematics and guaranteed to converge to the true pricing function. A sketch of proof is provided [here](https://github.com/differential-machine-learning/appendices/blob/master/App1-LSM.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYxcrxdTmrjS"
      },
      "source": [
        "## Black & Scholes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7EKPaXqmrjS"
      },
      "source": [
        "As a first, and somewhat compulsory example, we learn the pricing and risk function of a European call in Black & Scholes' classic model of 1973, training on simulated LSM samples and verifying performance against the correct pricing function, given by Black & Scholes formula.\n",
        "\n",
        "The simple code below generates a training set of LSM samples, along with pathwise differentials, and a test set of true prices and risks computed in closed form.\n",
        "\n",
        "The implementation is a fast, vectorized Monte-Carlo simulation in numpy. Code should be self explanatory, safe for two maybe unusual features. We allow raising volatility by a factor *volMult* between now (time $0$) and the pricing date $T_1$ to get more samples on the wings and better learn asymptotics. Our labels may also represent *the average of two antithetic samples* instead of one, which reduces variance for an acceptable cost in this simple case. \n",
        "\n",
        "Pathwise differentials are easily computed in Black & Scholes, where:\n",
        "\n",
        "$$\n",
        "    S_{T_2} = S_{T_1} exp \\left[ - \\frac{\\sigma^2}{2} \\left( {T_2} - {T_1} \\right) + \\sigma \\left( W_{T_2} - W_{T_1} \\right)  \\right]\n",
        "$$\n",
        "\n",
        "so\n",
        "\n",
        "$$\n",
        "    \\frac { \\partial \\left( S_{T_2} - K  \\right)^+} {S_{T_1}} = 1_{\\left\\{S_{T_2} > K\\right\\}} \\frac{S_{T_2}}{S_{T_1}}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw1YEvntmrjS"
      },
      "outputs": [],
      "source": [
        "# helper analytics    \n",
        "def bsPrice(spot, strike, vol, T):\n",
        "    d1 = (np.log(spot/strike) + 0.5 * vol * vol * T) / vol / np.sqrt(T)\n",
        "    d2 = d1 - vol * np.sqrt(T)\n",
        "    return spot * norm.cdf(d1) - strike * norm.cdf(d2)\n",
        "\n",
        "def bsDelta(spot, strike, vol, T):\n",
        "    d1 = (np.log(spot/strike) + 0.5 * vol * vol * T) / vol / np.sqrt(T)\n",
        "    return norm.cdf(d1)\n",
        "\n",
        "def bsVega(spot, strike, vol, T):\n",
        "    d1 = (np.log(spot/strike) + 0.5 * vol * vol * T) / vol / np.sqrt(T)\n",
        "    return spot * np.sqrt(T) * norm.pdf(d1)\n",
        "#\n",
        "    \n",
        "# main class\n",
        "class BlackScholes:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 vol=0.2,\n",
        "                 T1=1, \n",
        "                 T2=2, \n",
        "                 K=1.10,\n",
        "                 volMult=1.5):\n",
        "        \n",
        "        self.spot = 1\n",
        "        self.vol = vol\n",
        "        self.T1 = T1\n",
        "        self.T2 = T2\n",
        "        self.K = K\n",
        "        self.volMult = volMult\n",
        "                        \n",
        "    # training set: returns S1 (mx1), C2 (mx1) and dC2/dS1 (mx1)\n",
        "    def trainingSet(self, m, anti=True, seed=None):\n",
        "    \n",
        "        np.random.seed(seed)\n",
        "        \n",
        "        # 2 sets of normal returns\n",
        "        returns = np.random.normal(size=[m, 2])\n",
        "\n",
        "        # SDE\n",
        "        vol0 = self.vol * self.volMult\n",
        "        R1 = np.exp(-0.5*vol0*vol0*self.T1 + vol0*np.sqrt(self.T1)*returns[:,0])\n",
        "        R2 = np.exp(-0.5*self.vol*self.vol*(self.T2-self.T1) \\\n",
        "                    + self.vol*np.sqrt(self.T2-self.T1)*returns[:,1])\n",
        "        S1 = self.spot * R1\n",
        "        S2 = S1 * R2 \n",
        "\n",
        "        # payoff\n",
        "        pay = np.maximum(0, S2 - self.K)\n",
        "        \n",
        "        # two antithetic paths\n",
        "        if anti:\n",
        "            \n",
        "            R2a = np.exp(-0.5*self.vol*self.vol*(self.T2-self.T1) \\\n",
        "                    - self.vol*np.sqrt(self.T2-self.T1)*returns[:,1])\n",
        "            S2a = S1 * R2a             \n",
        "            paya = np.maximum(0, S2a - self.K)\n",
        "            \n",
        "            X = S1\n",
        "            Y = 0.5 * (pay + paya)\n",
        "    \n",
        "            # differentials\n",
        "            Z1 =  np.where(S2 > self.K, R2, 0.0).reshape((-1,1)) \n",
        "            Z2 =  np.where(S2a > self.K, R2a, 0.0).reshape((-1,1)) \n",
        "            Z = 0.5 * (Z1 + Z2)\n",
        "                    \n",
        "        # standard\n",
        "        else:\n",
        "        \n",
        "            X = S1\n",
        "            Y = pay\n",
        "            \n",
        "            # differentials\n",
        "            Z =  np.where(S2 > self.K, R2, 0.0).reshape((-1,1)) \n",
        "        \n",
        "        return X.reshape([-1,1]), Y.reshape([-1,1]), Z.reshape([-1,1])\n",
        "    \n",
        "    # test set: returns a grid of uniform spots \n",
        "    # with corresponding ground true prices, deltas and vegas\n",
        "    def testSet(self, lower=0.35, upper=1.65, num=100, seed=None):\n",
        "        \n",
        "        spots = np.linspace(lower, upper, num).reshape((-1, 1))\n",
        "        # compute prices, deltas and vegas\n",
        "        prices = bsPrice(spots, self.K, self.vol, self.T2 - self.T1).reshape((-1, 1))\n",
        "        deltas = bsDelta(spots, self.K, self.vol, self.T2 - self.T1).reshape((-1, 1))\n",
        "        vegas = bsVega(spots, self.K, self.vol, self.T2 - self.T1).reshape((-1, 1))\n",
        "        return spots, spots, prices, deltas, vegas   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0-ENPAsmrjT"
      },
      "source": [
        "The Black & Scholes example is not particularly interesting, but it allows to fix ideas in a simple context. The reason why it is not so interesting is its dimension 1. The only state variable is the spot price $S_{T_1}$. In low dimension, function approximation is an easy problem, effectively resolved by standard deep learning or classic regression or interpolation models. \n",
        "\n",
        "The test functions below train classical and differential networks and display test results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qNr1rdEmrjT"
      },
      "outputs": [],
      "source": [
        "def test(generator, \n",
        "         sizes, \n",
        "         nTest, \n",
        "         simulSeed=None, \n",
        "         testSeed=None, \n",
        "         weightSeed=None, \n",
        "         deltidx=0):\n",
        "\n",
        "    # simulation\n",
        "    print(\"simulating training, valid and test sets\")\n",
        "    xTrain, yTrain, dydxTrain = generator.trainingSet(max(sizes), seed=simulSeed)\n",
        "    xTest, xAxis, yTest, dydxTest, vegas = generator.testSet(num=nTest, seed=testSeed)\n",
        "    print(\"done\")\n",
        "\n",
        "    # neural approximator\n",
        "    print(\"initializing neural appropximator\")\n",
        "    regressor = Neural_Approximator(xTrain, yTrain, dydxTrain)\n",
        "    print(\"done\")\n",
        "    \n",
        "    predvalues = {}    \n",
        "    preddeltas = {}\n",
        "    for size in sizes:        \n",
        "            \n",
        "        print(\"\\nsize %d\" % size)\n",
        "        regressor.prepare(size, False, weight_seed=weightSeed)\n",
        "            \n",
        "        t0 = time.time()\n",
        "        regressor.train(\"standard training\")\n",
        "        predictions, deltas = regressor.predict_values_and_derivs(xTest)\n",
        "        predvalues[(\"standard\", size)] = predictions\n",
        "        preddeltas[(\"standard\", size)] = deltas[:, deltidx]\n",
        "        t1 = time.time()\n",
        "        \n",
        "        regressor.prepare(size, True, weight_seed=weightSeed)\n",
        "            \n",
        "        t0 = time.time()\n",
        "        regressor.train(\"differential training\")\n",
        "        predictions, deltas = regressor.predict_values_and_derivs(xTest)\n",
        "        predvalues[(\"differential\", size)] = predictions\n",
        "        preddeltas[(\"differential\", size)] = deltas[:, deltidx]\n",
        "        t1 = time.time()\n",
        "        \n",
        "    return xAxis, yTest, dydxTest[:, deltidx], vegas, predvalues, preddeltas\n",
        "\n",
        "def graph(title, \n",
        "          predictions, \n",
        "          xAxis, \n",
        "          xAxisName, \n",
        "          yAxisName, \n",
        "          targets, \n",
        "          sizes, \n",
        "          computeRmse=False, \n",
        "          weights=None):\n",
        "    \n",
        "    numRows = len(sizes)\n",
        "    numCols = 2\n",
        "\n",
        "    fig, ax = plt.subplots(numRows, numCols, squeeze=False)\n",
        "    fig.set_size_inches(4 * numCols + 1.5, 4 * numRows)\n",
        "\n",
        "    for i, size in enumerate(sizes):\n",
        "        ax[i,0].annotate(\"size %d\" % size, xy=(0, 0.5), \n",
        "          xytext=(-ax[i,0].yaxis.labelpad-5, 0),\n",
        "          xycoords=ax[i,0].yaxis.label, textcoords='offset points',\n",
        "          ha='right', va='center')\n",
        "  \n",
        "    ax[0,0].set_title(\"standard\")\n",
        "    ax[0,1].set_title(\"differential\")\n",
        "    \n",
        "    for i, size in enumerate(sizes):        \n",
        "        for j, regType, in enumerate([\"standard\", \"differential\"]):\n",
        "\n",
        "            if computeRmse:\n",
        "                errors = 100 * (predictions[(regType, size)] - targets)\n",
        "                if weights is not None:\n",
        "                    errors /= weights\n",
        "                rmse = np.sqrt((errors ** 2).mean(axis=0))\n",
        "                t = \"rmse %.2f\" % rmse\n",
        "            else:\n",
        "                t = xAxisName\n",
        "                \n",
        "            ax[i,j].set_xlabel(t)            \n",
        "            ax[i,j].set_ylabel(yAxisName)\n",
        "\n",
        "            ax[i,j].plot(xAxis*100, predictions[(regType, size)]*100, 'co', \\\n",
        "                         markersize=2, markerfacecolor='white', label=\"predicted\")\n",
        "            ax[i,j].plot(xAxis*100, targets*100, 'r.', markersize=0.5, label='targets')\n",
        "\n",
        "            ax[i,j].legend(prop={'size': 8}, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.suptitle(\"% s -- %s\" % (title, yAxisName), fontsize=16)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-A0nFJ-mrjU"
      },
      "source": [
        "The code below trains approximators on LSM samples simulated by our *BlackScholes* class. Classical deep learning is able to learn very accurate approximations in this simple case, so differential learning doesn't make much difference, although it improves the learned shape and differentials on small datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s7mWfYx8mrjU"
      },
      "outputs": [],
      "source": [
        "# simulation set sizes to perform\n",
        "sizes = [1024, 8192]\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "\n",
        "# seed\n",
        "# simulSeed = 1234\n",
        "simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "weightSeed = None\n",
        "\n",
        "# number of test scenarios\n",
        "nTest = 100    \n",
        "\n",
        "# go\n",
        "generator = BlackScholes()\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas = \\\n",
        "    test(generator, sizes, nTest, simulSeed, None, weightSeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHO8AOQ7mrjV"
      },
      "outputs": [],
      "source": [
        "# show predicitions\n",
        "graph(\"Black & Scholes\", values, xAxis, \"\", \"values\", yTest, sizes, True)\n",
        "\n",
        "# show deltas\n",
        "if showDeltas:\n",
        "    graph(\"Black & Scholes\", deltas, xAxis, \"\", \"deltas\", dydxTest, sizes, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbNPRaGJmrjW"
      },
      "source": [
        "## Gaussian basket options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Liy98_mrjW"
      },
      "source": [
        "The recent success of deep learning is largely due to their ability to break the long standing *curse of dimensionality* that breaks classic regression models in high dimension. Contrarily to classic linear models, neural networks don't regress on a fixed set of basis functions. They *learn from data* a relevant regression basis in their hidden layers, embedding a powerful dimension reduction capability in their structure. This is why deep learning succeeded in such high dimensional tasks as computer vision, where the dimension of the inputs is the number of pixels in a picture. Convolutional nets effectively learn the low dimensional features that matter to e.g. image recognition, something a classic regression model couldn't do.\n",
        "\n",
        "*Differential* machine learning also shines in high dimension, where differential labels help identify relevant features more effectively. In fact, the additional performance from differential training exponentially increases with dimension. This is why twin networks are particularly effective for learning values of complex transactions or trading books as functions of a high dimensional state. \n",
        "\n",
        "To illustrate this ability in a simple context, we extend the Black & Scholes example to a basket option written on $n$ correlated stocks. In place of the Black & Scholes model, the stocks are simulated in Bachelier's Gaussian model, where the true price of the basket option is known in closed form, and given by Bachelier's formula applied to the basket at $T_1$. Therefore, we can monitor the performance of our approximators by comparison to the correct, analytic  prices and deltas.\n",
        "\n",
        "In addition, the example is particularly interesting because *the price is really a non-linear function of the one-dimensional basket*. We expect the machine to learn that from data, and twin networks achieve this a lot better than feedforward networks. The stellar results of the section 3.1 of the article demonstrate the power of differential learning. \n",
        "\n",
        "The code below follows the exact same logic as the *BlackScholes* simulator to simulate LSM samples for a basket option in a $n$-dimensional, correlated Bachlier model. The correlations, volatility and basket weights are re-generated randomly on every run, allowing to verify performance in multiple configurations. It is a direct extension of the *BlackScholes* simulator and, as such, such be rather self explanatory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQP5y5vkmrjW"
      },
      "outputs": [],
      "source": [
        "# helper analytics\n",
        "def bachPrice(spot, strike, vol, T):\n",
        "    d = (spot - strike) / vol / np.sqrt(T)\n",
        "    return  vol * np.sqrt(T) * (d * norm.cdf(d) + norm.pdf(d))\n",
        "\n",
        "def bachDelta(spot, strike, vol, T):\n",
        "    d = (spot - strike) / vol / np.sqrt(T)\n",
        "    return norm.cdf(d)\n",
        "\n",
        "def bachVega(spot, strike, vol, T):\n",
        "    d = (spot - strike) / vol / np.sqrt(T)\n",
        "    return np.sqrt(T) * norm.pdf(d)\n",
        "#\n",
        "    \n",
        "# generates a random correlation matrix\n",
        "def genCorrel(n):\n",
        "    randoms = np.random.uniform(low=-1., high=1., size=(2*n, n))\n",
        "    cov = randoms.T @ randoms\n",
        "    invvols = np.diag(1. / np.sqrt(np.diagonal(cov)))\n",
        "    return np.linalg.multi_dot([invvols, cov, invvols])\n",
        "#\n",
        "\n",
        "class Bachelier:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 n,\n",
        "                 T1=1, \n",
        "                 T2=2, \n",
        "                 K=1.10,\n",
        "                 volMult=1.5):\n",
        "        \n",
        "        self.n = n\n",
        "        self.T1 = T1\n",
        "        self.T2 = T2\n",
        "        self.K = K\n",
        "        self.volMult = volMult\n",
        "                \n",
        "    # training set: returns S1 (mxn), C2 (mx1) and dC2/dS1 (mxn)\n",
        "    def trainingSet(self, m, anti=True, seed=None, bktVol=0.2):\n",
        "    \n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # spots all currently 1, without loss of generality\n",
        "        self.S0 = np.repeat(1., self.n)\n",
        "        # random correl\n",
        "        self.corr = genCorrel(self.n)\n",
        "\n",
        "        # random weights\n",
        "        self.a = np.random.uniform(low=1., high=10., size=self.n)\n",
        "        self.a /= np.sum(self.a)\n",
        "        # random vols\n",
        "        vols = np.random.uniform(low=5., high = 50., size = self.n)\n",
        "        # normalize vols for a given volatility of basket, \n",
        "        # helps with charts without loss of generality\n",
        "        avols = (self.a * vols).reshape((-1,1))\n",
        "        v = np.sqrt(np.linalg.multi_dot([avols.T, self.corr, avols]).reshape(1))\n",
        "        self.vols = vols * bktVol / v\n",
        "        self.bktVol = bktVol\n",
        "\n",
        "        # Choleski etc. for simulation\n",
        "        diagv = np.diag(self.vols)\n",
        "        self.cov = np.linalg.multi_dot([diagv, self.corr, diagv])\n",
        "        self.chol = np.linalg.cholesky(self.cov) * np.sqrt(self.T2 - self.T1)\n",
        "        # increase vols for simulation of X so we have more samples in the wings\n",
        "        self.chol0 = self.chol * self.volMult * np.sqrt(self.T1 / (self.T2 - self.T1))\n",
        "        # simulations\n",
        "        normals = np.random.normal(size=[2, m, self.n])\n",
        "        inc0 = normals[0, :, :] @ self.chol0.T\n",
        "        inc1 = normals[1, :, :] @ self.chol.T\n",
        "    \n",
        "        S1 = self.S0 + inc0\n",
        "        \n",
        "        S2 = S1 + inc1\n",
        "        bkt2 = np.dot(S2, self.a)\n",
        "        pay = np.maximum(0, bkt2 - self.K)\n",
        "\n",
        "        # two antithetic paths\n",
        "        if anti:\n",
        "            \n",
        "            S2a = S1 - inc1\n",
        "            bkt2a = np.dot(S2a, self.a)\n",
        "            paya = np.maximum(0, bkt2a - self.K)\n",
        "            \n",
        "            X = S1\n",
        "            Y = 0.5 * (pay + paya)\n",
        "    \n",
        "            # differentials\n",
        "            Z1 =  np.where(bkt2 > self.K, 1.0, 0.0).reshape((-1,1)) * self.a.reshape((1,-1))\n",
        "            Z2 =  np.where(bkt2a > self.K, 1.0, 0.0).reshape((-1,1)) * self.a.reshape((1,-1))\n",
        "            Z = 0.5 * (Z1 + Z2)\n",
        "                    \n",
        "        # standard\n",
        "        else:\n",
        "        \n",
        "            X = S1\n",
        "            Y = pay\n",
        "            \n",
        "            # differentials\n",
        "            Z =  np.where(bkt2 > self.K, 1.0, 0.0).reshape((-1,1)) * self.a.reshape((1,-1))\n",
        "            \n",
        "        return X, Y.reshape(-1,1), Z\n",
        "    \n",
        "    # test set: returns an array of independent, uniformly random spots \n",
        "    # with corresponding baskets, ground true prices, deltas and vegas\n",
        "    def testSet(self, lower=0.5, upper=1.50, num=4096, seed=None):\n",
        "        \n",
        "        np.random.seed(seed)\n",
        "        # adjust lower and upper for dimension\n",
        "        adj = 1 + 0.5 * np.sqrt((self.n-1)*(upper-lower)/12)\n",
        "        adj_lower = 1.0 - (1.0-lower) * adj\n",
        "        adj_upper = 1.0 + (upper - 1.0) * adj\n",
        "        # draw spots\n",
        "        spots = np.random.uniform(low=adj_lower, high = adj_upper, size=(num, self.n))\n",
        "        # compute baskets, prices, deltas and vegas\n",
        "        baskets = np.dot(spots, self.a).reshape((-1, 1))\n",
        "        prices = bachPrice(baskets, self.K, self.bktVol, self.T2 - self.T1).reshape((-1, 1))\n",
        "        deltas = bachDelta(baskets, self.K, self.bktVol, self.T2 - self.T1) @ self.a.reshape((1, -1))\n",
        "        vegas = bachVega(baskets, self.K, self.bktVol, self.T2 - self.T1) \n",
        "        return spots, baskets, prices, deltas, vegas    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLiRU1KLmrjX"
      },
      "source": [
        "We can reuse our testing functions without modification, starting with dimension 1, where the basket is a European call, priced in the Bachelier model, very similar to Black & Scholes. Unsurprisingly, the results are identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LDV6C02-mrjX"
      },
      "outputs": [],
      "source": [
        "# basket / bachelier dimension\n",
        "basketDim = 1\n",
        "\n",
        "# simulation set sizes to perform\n",
        "sizes = [1024, 8192]\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "deltidx = 0 # show delta to first stock\n",
        "\n",
        "# seed\n",
        "# simulSeed = 1234\n",
        "simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "testSeed = None\n",
        "weightSeed = None\n",
        "    \n",
        "# number of test scenarios\n",
        "nTest = 4096    \n",
        "\n",
        "# go\n",
        "generator = Bachelier(basketDim)\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas = \\\n",
        "    test(generator, sizes, nTest, simulSeed, None, weightSeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEeZEFeImrjX"
      },
      "outputs": [],
      "source": [
        "# show predicitions\n",
        "graph(\"Bachelier dimension %d\" % basketDim, values, xAxis, \"\", \"values\", yTest, sizes, True)\n",
        "\n",
        "# show deltas\n",
        "if showDeltas:\n",
        "    graph(\"Bachelier dimension %d\" % basketDim, deltas, xAxis, \"\", \"deltas\", dydxTest, sizes, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHVdfTc0mrjY"
      },
      "source": [
        "Moving on to the dimension 7 of the article, we display predictions and correct values as a function of the underlying basket at $T_1$, for 4096 independent examples. The thickness of the plot measures the ability to learn from data that the value is a fixed function of the current undelying basket. A thin curve reflects that this property is correctly learned. A thick line means that the approximator predicts different values for different sets of stocks corresponding to the same basket, hence, failing to learn the pricing function correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "utxHpAAtmrjY"
      },
      "outputs": [],
      "source": [
        "# basket / bachelier dimension\n",
        "basketDim = 7\n",
        "\n",
        "# simulation set sizes to perform\n",
        "sizes = [4096, 8192, 16384]\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "deltidx = 0 # show delta to first stock\n",
        "\n",
        "# seed\n",
        "# simulSeed = 1234\n",
        "simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "testSeed = None\n",
        "weightSeed = None\n",
        "    \n",
        "# number of test scenarios\n",
        "nTest = 4096    \n",
        "\n",
        "# go\n",
        "generator = Bachelier(basketDim)\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas = \\\n",
        "    test(generator, sizes, nTest, simulSeed, None, weightSeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "OwgKhNVimrjY"
      },
      "outputs": [],
      "source": [
        "# show predicitions\n",
        "graph(\"Bachelier dimension %d\" % basketDim, values, xAxis, \"\", \"values\", yTest, sizes, True)\n",
        "\n",
        "# show deltas\n",
        "if showDeltas:\n",
        "    graph(\"Bachelier dimension %d\" % basketDim, deltas, xAxis, \"\", \"deltas\", dydxTest, sizes, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvM0BeeWmrjY"
      },
      "source": [
        "We reproduce the result of the article where both networks converge to a correct approximation, but differential training gets there orders of magnitude faster, and especially outperforms on smaller training sets. This is what makes it so particularly relevant for the risk management of financial Derivatives.\n",
        "\n",
        "Below, we test dimension 20. Notice that learning time is virtually unaffected by dimension, and that the performance of the twin network is resilient to high dimensionality, where the standard network starts to struggle. \n",
        "\n",
        "Our simple implementation correctly works in dimension up to 30 something. The more thorough data preparation discussed in [this appendix](https://github.com/differential-machine-learning/appendices/blob/master/App2-Preprocessing.pdf) in necessary for higher dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PVt2z9tImrjY"
      },
      "outputs": [],
      "source": [
        "# basket / bachelier dimension\n",
        "basketDim = 50\n",
        "\n",
        "# simulation set sizes to perform\n",
        "sizes = [16384, 4*16384, 8*16384]\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "deltidx = 0 # show delta to first stock\n",
        "\n",
        "# seed\n",
        "# simulSeed = 1234\n",
        "simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "testSeed = None\n",
        "weightSeed = None\n",
        "    \n",
        "# number of test scenarios\n",
        "nTest = 4096    \n",
        "\n",
        "# go\n",
        "generator = Bachelier(basketDim)\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas = \\\n",
        "    test(generator, sizes, nTest, simulSeed, None, weightSeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "EXzFDB8XmrjZ"
      },
      "outputs": [],
      "source": [
        "# show predicitions\n",
        "graph(\"Bachelier dimension %d\" % basketDim, values, xAxis, \"\", \"values\", yTest, sizes, True)\n",
        "\n",
        "# show deltas\n",
        "if showDeltas:\n",
        "    graph(\"Bachelier dimension %d\" % basketDim, deltas, xAxis, \"\", \"deltas\", dydxTest, sizes, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMPb5rkgmrjZ"
      },
      "source": [
        "# Conclusion: Automatic Adjoint Differentiation (AAD)\n",
        "---\n",
        "\n",
        "We implemented the main ideas of the working paper and verified their unreasonable effectiveness for the approximation of pricing and risk functions in finance. Everything relies on *differential labels*, the gradients of training labels to training inputs, fed to the machine learning model in an augmented dataset. We have seen that training on differentials offers a massive performance improvement, but, of course, the differential labels must be computed first.\n",
        "\n",
        "In particularly simple textbook contexts, like a European call in Black & Scholes or a basket option in multi-dimensional Bachelier, differential labels are easily computed in explicit form. In low dimension, they could be computed by finite differences. In a general case with an arbitrary schedule of complex cash-flows simulated in an arbitrarily sophisticated model, closed form differentials are not available and finite differences are far too slow. In dimension 100, every training example must be computed 101 times to estimate differentials by finite differences. In addition, differentials approximated by finite differences may not be accurate enough for the purpose of training: we don't want the optimizer chasing imprecise differentials.\n",
        "\n",
        "Introduced to finance by the ground breaking *Smoking Adjoints* (Giles and Glasserman, Risk 2006), AAD is a game changing technology allowing to compute differentials of arbitrary computations, automatically, with analytic precision, and for a computation cost of around 2 to 5 times one evaluation, depending on implementation, and *independently on the dimension of the gradient*. \n",
        "\n",
        "AAD arguably constitutes the most significant progress in computational finance of the past 20 years. It gave us real-time risk reports for complex Derivatives trading books and regulations like XVA, as well as instantaneous calibrations. It made differentials massively available for research and development in finance. Quoting the conclusion of our Wilmott piece *Computation graphs for AAD and Machine Learning parts 1, 2 and 3* (Savine, Wilmott Magazine, 2019-2020):\n",
        "\n",
        "*New implementations of AAD are pushing the limits of its efficiency, while quantitative analysts are leveraging them in unexpected ways, besides the evident application to risk sensitivities or calibration.*\n",
        "\n",
        "To a large extent, differential machine learning is another strong application of AAD. It is AAD that gave us the massive number of accurate differentials necessary to implement it, for a very cheap computation cost, and is ultimately responsible for the spectacular performance improvement. The real-world examples in the Risk paper, sections 3.2 and 3.3, were trained on AAD differential labels.\n",
        "\n",
        "The working paper or the complements do not cover AAD. Readers are referred to the (stellar) founding paper. [This textbook](https://www.amazon.com/Modern-Computational-Finance-Parallel-Simulations-dp-1119539455/dp/1119539455) provides a complete, up to date overview of AAD, its applications in finance, and a complete, professional implementation in modern C++.\n",
        "\n",
        "The video tutorial below introduces the core ideas in 15 minutes. Click on the picture to play. \n",
        "\n",
        "[<img src=\"https://miro.medium.com/max/1400/1*bHsIA1jy08p71uZcg8HiIQ.png\" />](https://towardsdatascience.com/automatic-differentiation-15min-video-tutorial-with-application-in-machine-learning-and-finance-333e18c0ecbb)[Towards Data Science: AAD Explained in 15min](https://towardsdatascience.com/automatic-differentiation-15min-video-tutorial-with-application-in-machine-learning-and-finance-333e18c0ecbb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # SDE\n",
        "#         vol0 = self.vol * self.volMult\n",
        "#         R1 = np.exp(-0.5*vol0*vol0*self.T1 + vol0*np.sqrt(self.T1)*returns[:,0])\n",
        "#         R2 = np.exp(-0.5*self.vol*self.vol*(self.T2-self.T1) \\\n",
        "#                     + self.vol*np.sqrt(self.T2-self.T1)*returns[:,1])\n",
        "#         S1 = self.spot * R1\n",
        "#         S2 = S1 * R2 \n",
        "\n",
        "#         # payoff\n",
        "#         pay = np.maximum(0, S2 - self.K)\n",
        "        \n",
        "#         # two antithetic paths\n",
        "#         if anti:\n",
        "            \n",
        "#             R2a = np.exp(-0.5*self.vol*self.vol*(self.T2-self.T1) \\\n",
        "#                     - self.vol*np.sqrt(self.T2-self.T1)*returns[:,1])\n",
        "#             S2a = S1 * R2a             \n",
        "#             paya = np.maximum(0, S2a - self.K)\n",
        "            \n",
        "#             X = S1\n",
        "#             Y = 0.5 * (pay + paya)\n",
        "    \n",
        "#             # differentials\n",
        "#             Z1 =  np.where(S2 > self.K, R2, 0.0).reshape((-1,1)) \n",
        "#             Z2 =  np.where(S2a > self.K, R2a, 0.0).reshape((-1,1)) \n",
        "#             Z = 0.5 * (Z1 + Z2)\n",
        "                    \n",
        "#         # standard\n",
        "#         else:\n",
        "        \n",
        "#             X = S1\n",
        "#             Y = pay\n",
        "            \n",
        "#             # differentials\n",
        "#             Z =  np.where(S2 > self.K, R2, 0.0).reshape((-1,1)) \n",
        "        \n",
        "#         return X.reshape([-1,1]), Y.reshape([-1,1]), Z.reshape([-1,1])"
      ],
      "metadata": {
        "id": "jMPHiZyZ1kG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uoyu-Y_mrjZ"
      },
      "outputs": [],
      "source": [
        "class AutoCallBS:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 n,\n",
        "                 n_exercise,\n",
        "                 T1=1, \n",
        "                 Tm=4, \n",
        "                 K=[0.9, 0.8, 0.7, 0.6],\n",
        "                 cpn=0.03,\n",
        "                 volMult=1.5):\n",
        "        \n",
        "        self.n = n\n",
        "        self.n_exercise = n_exercise\n",
        "        self.T1 = T1\n",
        "        self.Tm = Tm\n",
        "        self.K = K\n",
        "        self.volMult = volMult\n",
        "        self.cpn = cpn\n",
        "                \n",
        "    # training set: returns S1 (mxn), AC2 (mx1) and dAC2/dS1 (mxn)\n",
        "    def trainingSet(self, m, anti=True, seed=None, bktVol=0.2):\n",
        "    \n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # spots all currently 1, without loss of generality\n",
        "        self.S0 = np.repeat(1., self.n)\n",
        "        # random correl\n",
        "        self.corr = genCorrel(self.n)\n",
        "\n",
        "        # random weights\n",
        "        self.a = np.random.uniform(low=1., high=10., size=self.n)\n",
        "        self.a /= np.sum(self.a)\n",
        "        # random vols\n",
        "        vols = np.random.uniform(low=5., high = 50., size = self.n)\n",
        "        # normalize vols for a given volatility of basket, \n",
        "        # helps with charts without loss of generality\n",
        "        avols = (self.a * vols).reshape((-1,1))\n",
        "        v = np.sqrt(np.linalg.multi_dot([avols.T, self.corr, avols]).reshape(1))\n",
        "        self.vols = vols * bktVol / v\n",
        "        self.bktVol = bktVol\n",
        "\n",
        "        # Choleski etc. for simulation\n",
        "        diagv = np.diag(self.vols)\n",
        "        self.cov = np.linalg.multi_dot([diagv, self.corr, diagv])\n",
        "        self.chol = np.linalg.cholesky(self.cov) * np.sqrt((self.Tm - self.T1)/self.n_exercise)\n",
        "        # increase vols for simulation of X so we have more samples in the wings\n",
        "        self.chol0 = self.chol * self.volMult * np.sqrt(self.T1/(self.Tm - self.T1)*self.n_exercise)\n",
        "        # simulations\n",
        "        normals = np.random.normal(size=[self.n_exercise + 1, m, self.n])\n",
        "        R1 = np.exp(-0.5*self.vols**2*self.T1 + normals[0, :, :] @ self.chol0.T)\n",
        "        S1 = self.S0 * R1\n",
        "        R_dic = {}\n",
        "        R_dic['R1'] = R1\n",
        "        S_dic = {}\n",
        "        S_dic['S1'] = S1\n",
        "        for t_i in range(self.n_exercise):\n",
        "            R_dic['R{}'.format(t_i+2)] = np.exp(-0.5*self.vols**2*(self.Tm - self.T1)/self.n_exercise + normals[t_i+1, :, :] @ self.chol.T)\n",
        "            S_dic['S{}'.format(t_i+2)] = S_dic['S{}'.format(t_i+1)] * R_dic['R{}'.format(t_i+2)]\n",
        "        \n",
        "        pay, unrefund_idx = self.payoff(S_dic)\n",
        "\n",
        "        # two antithetic paths\n",
        "        if anti:\n",
        "            Ra_dic = {}\n",
        "            Ra_dic['R1'] = R1\n",
        "            Sa_dic = {}\n",
        "            Sa_dic['S1'] = S1\n",
        "            for t_i in range(self.n_exercise):\n",
        "                Ra_dic['R{}'.format(t_i+2)] = np.exp(-0.5*self.vols**2*(self.Tm - self.T1)/self.n_exercise - normals[t_i+1, :, :] @ self.chol.T)\n",
        "                Sa_dic['S{}'.format(t_i+2)] = Sa_dic['S{}'.format(t_i+1)] * Ra_dic['R{}'.format(t_i+2)]\n",
        "            \n",
        "            paya, unrefund_idx_a = self.payoff(Sa_dic)\n",
        "            \n",
        "            X = S1\n",
        "            Y = 0.5 * (pay + paya)\n",
        "    \n",
        "            STm = S_dic['S{}'.format(self.n_exercise+1)]\n",
        "            STma = Sa_dic['S{}'.format(self.n_exercise+1)]\n",
        "\n",
        "            # differentials\n",
        "            Z1 = np.where(np.min(STm, axis=1).reshape((-1,1)) == STm, 1.0, 0.0) * unrefund_idx.reshape((-1,1))\n",
        "            Z2 = np.where(np.min(STma, axis=1).reshape((-1,1)) == STma, 1.0, 0.0) * unrefund_idx.reshape((-1,1))\n",
        "            \n",
        "            Z = 0.5 * (Z1 + Z2)\n",
        "                    \n",
        "        # standard\n",
        "        else:\n",
        "        \n",
        "            X = S1\n",
        "            Y = pay\n",
        "            \n",
        "            # differentials\n",
        "            Z =  np.where(np.min(STm, axis=1).reshape((-1,1)) == STm, 1.0, 0.0) * unrefund_idx.reshape((-1,1))\n",
        "            \n",
        "        return X, Y.reshape(-1,1), Z\n",
        "\n",
        "    def payoff(self, S_dic):\n",
        "        #check validation\n",
        "        if len(S_dic) != self.n_exercise:\n",
        "            print(\"path generate error\")\n",
        "            exit()\n",
        "        m = len(S_dic['S1'])\n",
        "        norefund_idx = np.ones_like(S_dic['S1'][:, 0])\n",
        "        pay = np.zeros_like(S_dic['S1'][:, 0])\n",
        "        for i, key in enumerate(S_dic):\n",
        "            if key != 'S1':\n",
        "                price = S_dic[key]\n",
        "                price *= norefund_idx.reshape((m,1))\n",
        "                refund_idx = np.all(np.maximum(0, price-self.K[i-1]*S_dic['S1']), axis=1).astype(np.float64)\n",
        "                pay += refund_idx * (1+self.cpn*(self.Tm-self.T1)/self.n_exercise*i)\n",
        "                norefund_idx -= refund_idx\n",
        "        pay += np.min(price * norefund_idx.reshape((m,1)), axis=1)\n",
        "        return pay, norefund_idx\n",
        "\n",
        "\n",
        "\n",
        "    # test set: returns an array of independent, uniformly random spots \n",
        "    # with corresponding baskets, ground true prices, deltas and vegas\n",
        "    def testSet(self, lower=0.5, upper=1.50, num=4096, seed=None):\n",
        "        \n",
        "        np.random.seed(seed)\n",
        "        # adjust lower and upper for dimension\n",
        "        adj = 1 + 0.5 * np.sqrt((self.n-1)*(upper-lower)/12)\n",
        "        adj_lower = 1.0 - (1.0-lower) * adj\n",
        "        adj_upper = 1.0 + (upper - 1.0) * adj\n",
        "        # draw spots\n",
        "        spots = np.random.uniform(low=adj_lower, high = adj_upper, size=(num, self.n))\n",
        "        # compute prices, deltas and vegas\n",
        "        \n",
        "        prices = 0\n",
        "        deltas = 0\n",
        "        vegas = 0\n",
        "        return spots, spots, prices, deltas, vegas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# simulation set sizes to perform\n",
        "sizes = [1024, 8192]\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "deltidx = 0 # show delta to first stock\n",
        "\n",
        "# seed\n",
        "# simulSeed = 1234\n",
        "simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "testSeed = None\n",
        "weightSeed = None\n",
        "    \n",
        "# number of test scenarios\n",
        "nTest = 4096    \n",
        "\n",
        "# go\n",
        "generator = AutoCallBS(3, 4)\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas = \\\n",
        "    test(generator, sizes, nTest, simulSeed, None, weightSeed)"
      ],
      "metadata": {
        "id": "qBFjZtSQYXaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}