{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moo-337/differential-machine-learning/blob/Auto-Call/DifferentialMLTF2_AutoCall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH28r65zmri7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/differential-machine-learning/notebooks/blob/master/DifferentialMLTF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icd9ZKB6mrjB"
      },
      "source": [
        "Working paper: https://arxiv.org/abs/2005.02347 \n",
        "GitHub: https://github.com/differential-machine-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL7p_WcDmrjC"
      },
      "source": [
        "# Differential Deep Learning\n",
        "\n",
        "---\n",
        "Updated for TensorFlow 2\n",
        "---\n",
        "\n",
        "---\n",
        "Antoine Savine, January 2021\n",
        "---\n",
        "\n",
        "This notebook implements the novel ideas of *twin networks* and *differential training* from the working paper [Differential Machine Learning](https://arxiv.org/abs/2005.02347) by Brian Huge and [Antoine Savine](https://antoinesavine.com) (2020), and applies them in a few simple contexts, including the reproduction of some results from the paper. In the article, we presented the ideas and discussed the algorithms, but skipped important implementation details best covered in a notebook with code. Those details are dicussed here.    \n",
        "\n",
        "Although this is a *demonstration* notebook, the algorithms are implemented in self contained, modular and extensible blocks of code. They may be easily extracted, extended and integrated into production systems. Some advanced extensions particularly important in a production context are discussed in the [appendices](https://github.com/differential-machine-learning/appendices/).\n",
        "\n",
        "The code is based on TensorFlow 1.x. You may run the notebook in a local TensorFlow 1.x enviroment *with GPU support*, also with common libraries like numpy and matplotlib. On Google Colab, the correct version of TensorFlow is automatically selected, and all the necessary libraries are preinstalled. All you need is *enable GPU support*: menu runtime/change runtime type and select hardware accelerator GPU.\n",
        "\n",
        "We do not further discuss the algorithms covered in the article or fundamentals of TensorFlow (see classic resources like [Geron's textbook](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291), first edition for TensorFlow 1.x). For modern deep learning and best practices, we refer e.g. to [Goodfellow and al's Deep Learning Book](https://www.deeplearningbook.org/) or [Andrew Ng's deep learning lectures](https://www.coursera.org/specializations/deep-learning). \n",
        "\n",
        "This notebook was ported on TensorFlow 2 in January 2021. The original TensorFlow 1 notebook will no longer be updated. For now, the notebook runs TensorFlow 1 code in a TensorFlow 2 environment. An upgrade to idiomatic TensorFlow 2 code will be available shortly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQWN6m4tmrjD"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9RvNyZtcmrjE",
        "outputId": "6e09b656-ee87-4c20-f879-3efd7a760298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version = 2.8.2\n",
            "GPU support =  True\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    %matplotlib notebook\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# import and test\n",
        "import tensorflow as tf2\n",
        "print(\"TF version =\", tf2.__version__)\n",
        "\n",
        "# we want TF 2.x\n",
        "assert tf2.__version__ >= \"2.0\"\n",
        "\n",
        "# disable eager execution etc\n",
        "tf = tf2.compat.v1\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "# disable annoying warnings\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# make sure we have GPU support\n",
        "print(\"GPU support = \", tf.test.is_gpu_available())\n",
        "\n",
        "# import other useful libs\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "import copy\n",
        "\n",
        "# representation of real numbers in TF, change here for 32/64 bits\n",
        "real_type = tf.float32\n",
        "# real_type = tf.float64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_kG4MMCmrjH"
      },
      "source": [
        "# Part I : Core Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsSye-jvmrjI"
      },
      "source": [
        "## Feedforward neural network in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wHb07vTmrjI"
      },
      "source": [
        "This function builds a classic feedforward neural network in TensorFlow, implementing the set of equations $(3)$ from the article. This is classic deep learning code and a direct translation of equations $(3)$ in Python. We tried to keep notations consistent with the paper.\n",
        "\n",
        "The weights are initialized with TensorFlow's  [variance_scaling_initializer](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/initializers/variance_scaling), implementing the particularly effective [Xavier/Glorot](https://www.deeplearning.ai/ai-notes/initialization/)  initialization strategy, a best practice in modern deep learning. A correct initialization is a key ingredient in an effective practical implementation of deep learning. \n",
        "\n",
        "Since the network is initialized randomly, we also implement seed in the interest of reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HBxxCK23mrjJ"
      },
      "outputs": [],
      "source": [
        "def vanilla_net(\n",
        "    input_dim,      # dimension of inputs, e.g. 10\n",
        "    hidden_units,   # units in hidden layers, assumed constant, e.g. 20\n",
        "    hidden_layers,  # number of hidden layers, e.g. 4\n",
        "    seed):          # seed for initialization or None for random\n",
        "    \n",
        "    # set seed\n",
        "    tf.set_random_seed(seed)\n",
        "    \n",
        "    # input layer\n",
        "    xs = tf.placeholder(shape=[None, input_dim], dtype=real_type)\n",
        "    \n",
        "    # connection weights and biases of hidden layers\n",
        "    ws = [None]\n",
        "    bs = [None]\n",
        "    # layer 0 (input) has no parameters\n",
        "    \n",
        "    # layer 0 = input layer\n",
        "    zs = [xs] # eq.3, l=0\n",
        "    \n",
        "    # first hidden layer (index 1)\n",
        "    # weight matrix\n",
        "    ws.append(tf.get_variable(\"w1\", [input_dim, hidden_units], \\\n",
        "        initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "    # bias vector\n",
        "    bs.append(tf.get_variable(\"b1\", [hidden_units], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # graph\n",
        "    zs.append(zs[0] @ ws[1] + bs[1]) # eq. 3, l=1\n",
        "    \n",
        "    # second hidden layer (index 2) to last (index hidden_layers)\n",
        "    for l in range(1, hidden_layers): \n",
        "        ws.append(tf.get_variable(\"w%d\"%(l+1), [hidden_units, hidden_units], \\\n",
        "            initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "        bs.append(tf.get_variable(\"b%d\"%(l+1), [hidden_units], \\\n",
        "            initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "        zs.append(tf.nn.softplus(zs[l]) @ ws[l+1] + bs[l+1]) # eq. 3, l=2..L-1\n",
        "\n",
        "    # output layer (index hidden_layers+1)\n",
        "    ws.append(tf.get_variable(\"w\"+str(hidden_layers+1), [hidden_units, 1], \\\n",
        "            initializer = tf.variance_scaling_initializer(), dtype=real_type))\n",
        "    bs.append(tf.get_variable(\"b\"+str(hidden_layers+1), [1], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # eq. 3, l=L\n",
        "    zs.append(tf.nn.softplus(zs[hidden_layers]) @ ws[hidden_layers+1] + bs[hidden_layers+1]) \n",
        "    \n",
        "    # result = output layer\n",
        "    ys = zs[hidden_layers+1]\n",
        "    \n",
        "    # return input layer, (parameters = weight matrices and bias vectors), \n",
        "    # [all layers] and output layer\n",
        "    return xs, (ws, bs), zs, ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmyLZWn2mrjK"
      },
      "source": [
        "## Explicit backpropagation and twin network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8JUvb-6mrjK"
      },
      "source": [
        "These functions construct the twin network by explicit backpropagation (eq. $(4)$ in the article). The twin network simultaneously and efficiently predicts values and their differentials to inputs, allowing to train on datasets augmented with differential labels (i.e. gradients of labels to inputs, as explained in the working paper). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GkL1n1gxmrjL"
      },
      "outputs": [],
      "source": [
        "# compute d_output/d_inputs by (explicit) backprop in vanilla net\n",
        "def backprop(\n",
        "    weights_and_biases, # 2nd output from vanilla_net() \n",
        "    zs):                # 3rd output from vanilla_net()\n",
        "    \n",
        "    ws, bs = weights_and_biases\n",
        "    L = len(zs) - 1\n",
        "    \n",
        "    # backpropagation, eq. 4, l=L..1\n",
        "    zbar = tf.ones_like(zs[L]) # zbar_L = 1\n",
        "    for l in range(L-1, 0, -1):\n",
        "        zbar = (zbar @ tf.transpose(ws[l+1])) * tf.nn.sigmoid(zs[l]) # eq. 4\n",
        "    # for l=0\n",
        "    zbar = zbar @ tf.transpose(ws[1]) # eq. 4\n",
        "    \n",
        "    xbar = zbar # xbar = zbar_0\n",
        "    \n",
        "    # dz[L] / dx\n",
        "    return xbar    \n",
        "\n",
        "# combined graph for valuation and differentiation\n",
        "def twin_net(input_dim, hidden_units, hidden_layers, seed):\n",
        "    \n",
        "    # first, build the feedforward net\n",
        "    xs, (ws, bs), zs, ys = vanilla_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # then, build its differentiation by backprop\n",
        "    xbar = backprop((ws, bs), zs)\n",
        "    \n",
        "    # return input x, output y and differentials d_y/d_z\n",
        "    return xs, ys, xbar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZqSQHucmrjL"
      },
      "source": [
        "## Vanilla training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkx0LS6mrjM"
      },
      "source": [
        "These are classic training loops for the feedforward neural network. As customary in modern deep learning, the training set is traversed in mini-batches, where the cost function is minimized with the best practice [ADAM](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/AdamOptimizer) algorithm. Andrew Ng's deeplearning.ai web site has an [intuitive presentation of various optimization algorithms in deep learning](https://www.deeplearning.ai/ai-notes/optimization/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TykxHUJymrjM"
      },
      "outputs": [],
      "source": [
        "def vanilla_training_graph(input_dim, hidden_units, hidden_layers, seed):\n",
        "    \n",
        "    # net\n",
        "    inputs, weights_and_biases, layers, predictions = \\\n",
        "        vanilla_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # backprop even though we are not USING differentials for training\n",
        "    # we still need them to predict derivatives dy_dx \n",
        "    derivs_predictions = backprop(weights_and_biases, layers)\n",
        "    \n",
        "    # placeholder for labels\n",
        "    labels = tf.placeholder(shape=[None, 1], dtype=real_type)\n",
        "    \n",
        "    # loss \n",
        "    loss = tf.losses.mean_squared_error(labels, predictions)\n",
        "    \n",
        "    # optimizer\n",
        "    learning_rate = tf.placeholder(real_type)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    \n",
        "    # return all necessary \n",
        "    return inputs, labels, predictions, derivs_predictions, learning_rate, loss, optimizer.minimize(loss)\n",
        "\n",
        "# training loop for one epoch\n",
        "def vanilla_train_one_epoch(# training graph from vanilla_training_graph()\n",
        "                            inputs, labels, lr_placeholder, minimizer,   \n",
        "                            # training set \n",
        "                            x_train, y_train,                           \n",
        "                            # params, left to client code\n",
        "                            learning_rate, batch_size, session):        \n",
        "    \n",
        "    m, n = x_train.shape\n",
        "    \n",
        "    # minimization loop over mini-batches\n",
        "    first = 0\n",
        "    last = min(batch_size, m)\n",
        "    while first < m:\n",
        "        session.run(minimizer, feed_dict = {\n",
        "            inputs: x_train[first:last], \n",
        "            labels: y_train[first:last],\n",
        "            lr_placeholder: learning_rate\n",
        "        })\n",
        "        first = last\n",
        "        last = min(first + batch_size, m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK09hmjcmrjM"
      },
      "source": [
        "## Differential training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb_2921xmrjM"
      },
      "source": [
        "The apparently similar *differential* training loop implements the main idea from the paper, to train twin networks on datasets augmented with differentials of labels to inputs, by minimization of a combined cost function reflecting errors in both predicted values *and predicted derivatives*:\n",
        "\n",
        "$$\n",
        "    c = \\alpha c_{\\text{val}} + \\beta c_{\\text{diff}}\n",
        "$$\n",
        "\n",
        "where $c_{\\text{val}}$ is the classic mean square error (MSE) of predictions to labels and $c_{\\text{diff}}$ is the cost of wrong derivatives:\n",
        "\n",
        "$$\n",
        "    c_{\\text{diff}} = \\frac{\\sum_{\\text{inputs j}} \\lambda_j^2 {mse}_j}{\\text{num inputs}}\n",
        "$$\n",
        "\n",
        "where ${mse}_j$ is the mean squared error of derivatives to input $j$ and the weights $\\lambda_j$ normalize derivatives so all the components of the cost have similar magnitudes, as explained in the article. We provide a basic normalization formula for the weights a few cells below, and a complete review of data preparation and weighting in [this appendix](https://github.com/differential-machine-learning/appendices/blob/master/App2-Preprocessing.pdf).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bGLZK34YmrjN"
      },
      "outputs": [],
      "source": [
        "def diff_training_graph(\n",
        "    # same as vanilla\n",
        "    input_dim, \n",
        "    hidden_units, \n",
        "    hidden_layers, \n",
        "    seed, \n",
        "    # balance relative weight of values and differentials \n",
        "    # loss = alpha * MSE(values) + beta * MSE(greeks, lambda_j) \n",
        "    # see online appendix\n",
        "    alpha, \n",
        "    beta,\n",
        "    lambda_j):\n",
        "    \n",
        "    # net, now a twin\n",
        "    inputs, predictions, derivs_predictions = twin_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # placeholder for labels, now also derivs labels\n",
        "    labels = tf.placeholder(shape=[None, 1], dtype=real_type)\n",
        "    derivs_labels = tf.placeholder(shape=[None, derivs_predictions.shape[1]], dtype=real_type)\n",
        "    \n",
        "    # loss, now combined values + derivatives\n",
        "    loss = alpha * tf.losses.mean_squared_error(labels, predictions) \\\n",
        "    + beta * tf. losses.mean_squared_error(derivs_labels * lambda_j, derivs_predictions * lambda_j)\n",
        "    \n",
        "    # optimizer, as vanilla\n",
        "    learning_rate = tf.placeholder(real_type)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    \n",
        "    # return all necessary tensors, including derivatives\n",
        "    # predictions and labels\n",
        "    return inputs, labels, derivs_labels, predictions, derivs_predictions, \\\n",
        "            learning_rate, loss, optimizer.minimize(loss)\n",
        "\n",
        "def diff_train_one_epoch(inputs, labels, derivs_labels, \n",
        "                         # graph\n",
        "                         lr_placeholder, minimizer,             \n",
        "                         # training set, extended\n",
        "                         x_train, y_train, dydx_train,          \n",
        "                         # params\n",
        "                         learning_rate, batch_size, session):   \n",
        "    \n",
        "    m, n = x_train.shape\n",
        "    \n",
        "    # minimization loop, now with Greeks\n",
        "    first = 0\n",
        "    last = min(batch_size, m)\n",
        "    while first < m:\n",
        "        session.run(minimizer, feed_dict = {\n",
        "            inputs: x_train[first:last], \n",
        "            labels: y_train[first:last],\n",
        "            derivs_labels: dydx_train[first:last],\n",
        "            lr_placeholder: learning_rate\n",
        "        })\n",
        "        first = last\n",
        "        last = min(first + batch_size, m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEwj2FUfmrjN"
      },
      "source": [
        "## Combined outer training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q34b2CtImrjO"
      },
      "source": [
        "The outer training loop optimizes the weights of neural approximators for a number of epochs (complete sweeps of the training set). We applied the recent [one-cycle learning rate schedule](https://arxiv.org/abs/1803.09820) of Leslie Smith and found that it considerably accelerates and stabilizes the training of neural networks. 100 epochs is more than sufficient in most practical cases. A convergence and/or cross-validation test may be included for early stopping. Typical training takes around a second on a decent GPU (longer on Colab's shared GPUs). The approximator class, defined next, holds all the necessary data and parameters, along with the TensorFlow graph and computing session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1COeYOwOmrjO"
      },
      "outputs": [],
      "source": [
        "def train(description,\n",
        "          # neural approximator\n",
        "          approximator,              \n",
        "          # training params\n",
        "          reinit=True, \n",
        "          epochs=100, \n",
        "          # one-cycle learning rate schedule\n",
        "          learning_rate_schedule=[    (0.0, 1.0e-8), \\\n",
        "                                      (0.2, 0.1),    \\\n",
        "                                      (0.6, 0.01),   \\\n",
        "                                      (0.9, 1.0e-6), \\\n",
        "                                      (1.0, 1.0e-8)  ], \n",
        "          batches_per_epoch=16,\n",
        "          min_batch_size=256,\n",
        "          # callback function and when to call it\n",
        "          callback=None,           # arbitrary callable\n",
        "          callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "    # batching\n",
        "    batch_size = max(min_batch_size, approximator.m // batches_per_epoch)\n",
        "    \n",
        "    # one-cycle learning rate sechedule\n",
        "    lr_schedule_epochs, lr_schedule_rates = zip(*learning_rate_schedule)\n",
        "            \n",
        "    # reset\n",
        "    if reinit:\n",
        "        approximator.session.run(approximator.initializer)\n",
        "    \n",
        "    # callback on epoch 0, if requested\n",
        "    if callback and 0 in callback_epochs:\n",
        "        callback(approximator, 0)\n",
        "        \n",
        "    # loop on epochs, with progress bar (tqdm)\n",
        "    for epoch in tqdm_notebook(range(epochs), desc=description):\n",
        "        \n",
        "        # interpolate learning rate in cycle\n",
        "        learning_rate = np.interp(epoch / epochs, lr_schedule_epochs, lr_schedule_rates)\n",
        "        \n",
        "        # train one epoch\n",
        "        \n",
        "        if not approximator.differential:\n",
        "        \n",
        "            vanilla_train_one_epoch(\n",
        "                approximator.inputs, \n",
        "                approximator.labels, \n",
        "                approximator.learning_rate, \n",
        "                approximator.minimizer, \n",
        "                approximator.x, \n",
        "                approximator.y, \n",
        "                learning_rate, \n",
        "                batch_size, \n",
        "                approximator.session)\n",
        "        \n",
        "        else:\n",
        "        \n",
        "            diff_train_one_epoch(\n",
        "                approximator.inputs, \n",
        "                approximator.labels, \n",
        "                approximator.derivs_labels,\n",
        "                approximator.learning_rate, \n",
        "                approximator.minimizer, \n",
        "                approximator.x, \n",
        "                approximator.y, \n",
        "                approximator.dy_dx,\n",
        "                learning_rate, \n",
        "                batch_size, \n",
        "                approximator.session)\n",
        "        \n",
        "        # callback, if requested\n",
        "        if callback and epoch in callback_epochs:\n",
        "            callback(approximator, epoch)\n",
        "\n",
        "    # final callback, if requested\n",
        "    if callback and epochs in callback_epochs:\n",
        "        callback(approximator, epochs)        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnmDGIYvmrjP"
      },
      "source": [
        "## Data normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGu-6xGlmrjP"
      },
      "source": [
        "The practical performance of neural networks strongly depends on implementation details, like weight initialization and optimization. Another crucial practicality is the *normalization of training data*. We refer to deep learning texbooks for a discussion of the importance of normalization. One reason is that we need hyperparameters like the learning rate schedule to remain constant over datasets. If notional was to be increased by factor 1M all things equal, gradients would be multiplied by 1M too and learning rates would have to be divided by 1M to keep things similar. Normalizing data avoids manual tinkering of hyperparameters for different datasets. All the examples in the paper: the Gaussian basket, the autocallable trade and the netting set, were all approximated with the exact same hyperparameters. This is only possible with normalized datasets. \n",
        "\n",
        "We implement below a *basic* normalization strategy, where the training inputs and labels are normalized by mean and standard deviation, with differentials normalized accordingly. The differential weights in the cost function $\\lambda_j$ divide costs by the norm of the normalized differentials, keeping similar the magnitude of all the components of the cost.\n",
        "\n",
        "We can do a lot better, especially with differential labels. We could perform successive changes of basis, combined with filtering strategies, to feed neural networks with data orthonormal in terms of inputs *and* in terms of differentials, highlighting the most significant risk factors and filtering out the irrelevant ones. This is covered in detail in the [this appendix](https://github.com/differential-machine-learning/appendices/blob/master/App2-Preprocessing.pdf), where we discuss novel, and extremely effective data preparation algorithms, exploiting information contained in differential labels to considerably alleviate the load of training neural networks and subsequently, improve training performance and reduce sensitivity to hyperparameters. \n",
        "\n",
        "Although the basic normalization implemented here is sufficient for textbook examples, a reliable implementation in production for arbitrary schedules of cash-flows, resilient to very high dimensionality, necessitatesthe more complete data preparation.\n",
        "\n",
        "Note that the prediction of values and derivatives must be adjusted accordingly: prediction inputs must be normalized, and resulting predictions must be 'un-normalized'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K5Y8H7bGmrjQ"
      },
      "outputs": [],
      "source": [
        "# basic data preparation\n",
        "epsilon = 1.0e-08\n",
        "def normalize_data(x_raw, y_raw, dydx_raw=None, crop=None):\n",
        "    \n",
        "    # crop dataset\n",
        "    m = crop if crop is not None else x_raw.shape[0]\n",
        "    x_cropped = x_raw[:m]\n",
        "    y_cropped = y_raw[:m]\n",
        "    dycropped_dxcropped = dydx_raw[:m] if dydx_raw is not None else None\n",
        "    \n",
        "    # normalize dataset\n",
        "    x_mean = x_cropped.mean(axis=0)\n",
        "    x_std = x_cropped.std(axis=0) + epsilon\n",
        "    x = (x_cropped- x_mean) / x_std\n",
        "    y_mean = y_cropped.mean(axis=0)\n",
        "    y_std = y_cropped.std(axis=0) + epsilon\n",
        "    y = (y_cropped-y_mean) / y_std\n",
        "    \n",
        "    # normalize derivatives too\n",
        "    if dycropped_dxcropped is not None:\n",
        "        dy_dx = dycropped_dxcropped / y_std * x_std \n",
        "        # weights of derivatives in cost function = (quad) mean size\n",
        "        lambda_j = 1.0 / np.sqrt((dy_dx ** 2).mean(axis=0)).reshape(1, -1)\n",
        "    else:\n",
        "        dy_dx = None\n",
        "        lambda_j = None\n",
        "    \n",
        "    return x_mean, x_std, x, y_mean, y_std, y, dy_dx, lambda_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scvIqupAmrjQ"
      },
      "source": [
        "## Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMzNB3n8mrjQ"
      },
      "source": [
        "For convenience, we put it all together in a *Neural_Approximator* class. Most of the code should be self explanatory. \n",
        "\n",
        "Note that we compute the coefficients $\\alpha$ and $\\beta$ for balancing cost between values and derivatives in a straightforward manner:\n",
        "\n",
        "$$\n",
        "    \\alpha = \\frac{1}{1 + \\lambda n} \\text{ and }  \\beta = \\frac{\\lambda n}{1 + \\lambda n}\n",
        "$$\n",
        "\n",
        "where $n$ is the number of inputs, so an error on a derivative has a weight similar to a value error, and $\\lambda$ is a hyperparameter without significant effect, as explained in the paper, and left to 1, safe for debugging.\n",
        "\n",
        "We implement a simple, feedforward architecture with 4 hidden layers of 20 units, throughout our tests. We kept the same architecture in production, with the addition of a *wide and deep* layer, inspired by [Google](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) and discussed in the [this appendix](https://github.com/differential-machine-learning/appendices/blob/master/App4-UnsupervisedTraining.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wXp2F-bImrjR"
      },
      "outputs": [],
      "source": [
        "class Neural_Approximator():\n",
        "    \n",
        "    def __init__(self, x_raw, y_raw, \n",
        "                 dydx_raw=None):      # derivatives labels, \n",
        "       \n",
        "        self.x_raw = x_raw\n",
        "        self.y_raw = y_raw\n",
        "        self.dydx_raw = dydx_raw\n",
        "        \n",
        "        # tensorflow logic\n",
        "        self.graph = None\n",
        "        self.session = None\n",
        "                        \n",
        "    def __del__(self):\n",
        "        if self.session is not None:\n",
        "            self.session.close()\n",
        "        \n",
        "    def build_graph(self,\n",
        "                differential,       # differential or not           \n",
        "                lam,                # balance cost between values and derivs  \n",
        "                hidden_units, \n",
        "                hidden_layers, \n",
        "                weight_seed):\n",
        "        \n",
        "        # first, deal with tensorflow logic\n",
        "        if self.session is not None:\n",
        "            self.session.close()\n",
        "\n",
        "        self.graph = tf.Graph()\n",
        "        \n",
        "        with self.graph.as_default():\n",
        "        \n",
        "            # build the graph, either vanilla or differential\n",
        "            self.differential = differential\n",
        "            \n",
        "            if not differential:\n",
        "            # vanilla \n",
        "                \n",
        "                self.inputs, \\\n",
        "                self.labels, \\\n",
        "                self.predictions, \\\n",
        "                self.derivs_predictions, \\\n",
        "                self.learning_rate, \\\n",
        "                self.loss, \\\n",
        "                self.minimizer \\\n",
        "                = vanilla_training_graph(self.n, hidden_units, hidden_layers, weight_seed)\n",
        "                    \n",
        "            else:\n",
        "            # differential\n",
        "            \n",
        "                if self.dy_dx is None:\n",
        "                    raise Exception(\"No differential labels for differential training graph\")\n",
        "            \n",
        "                self.alpha = 1.0 / (1.0 + lam * self.n)\n",
        "                self.beta = 1.0 - self.alpha\n",
        "                \n",
        "                self.inputs, \\\n",
        "                self.labels, \\\n",
        "                self.derivs_labels, \\\n",
        "                self.predictions, \\\n",
        "                self.derivs_predictions, \\\n",
        "                self.learning_rate, \\\n",
        "                self.loss, \\\n",
        "                self.minimizer = diff_training_graph(self.n, hidden_units, \\\n",
        "                                                     hidden_layers, weight_seed, \\\n",
        "                                                     self.alpha, self.beta, self.lambda_j)\n",
        "        \n",
        "            # global initializer\n",
        "            self.initializer = tf.global_variables_initializer()\n",
        "            \n",
        "        # done\n",
        "        self.graph.finalize()\n",
        "        self.session = tf.Session(graph=self.graph)\n",
        "                        \n",
        "    # prepare for training with m examples, standard or differential\n",
        "    def prepare(self, \n",
        "                m, \n",
        "                differential,\n",
        "                lam=1,              # balance cost between values and derivs  \n",
        "                # standard architecture\n",
        "                hidden_units=20, \n",
        "                hidden_layers=4, \n",
        "                weight_seed=None):\n",
        "\n",
        "        # prepare dataset\n",
        "        self.x_mean, self.x_std, self.x, self.y_mean, self.y_std, self.y, self.dy_dx, self.lambda_j = \\\n",
        "            normalize_data(self.x_raw, self.y_raw, self.dydx_raw, m)\n",
        "        \n",
        "        # build graph        \n",
        "        self.m, self.n = self.x.shape        \n",
        "        self.build_graph(differential, lam, hidden_units, hidden_layers, weight_seed)\n",
        "        \n",
        "    def train(self,            \n",
        "              description=\"training\",\n",
        "              # training params\n",
        "              reinit=True, \n",
        "              epochs=100, \n",
        "              # one-cycle learning rate schedule\n",
        "              learning_rate_schedule=[\n",
        "                  (0.0, 1.0e-8), \n",
        "                  (0.2, 0.1), \n",
        "                  (0.6, 0.01), \n",
        "                  (0.9, 1.0e-6), \n",
        "                  (1.0, 1.0e-8)], \n",
        "              batches_per_epoch=16,\n",
        "              min_batch_size=256,\n",
        "              # callback and when to call it\n",
        "              # we don't use callbacks, but this is very useful, e.g. for debugging\n",
        "              callback=None,           # arbitrary callable\n",
        "              callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "        train(description, \n",
        "              self, \n",
        "              reinit, \n",
        "              epochs, \n",
        "              learning_rate_schedule, \n",
        "              batches_per_epoch, \n",
        "              min_batch_size,\n",
        "              callback, \n",
        "              callback_epochs)\n",
        "     \n",
        "    def predict_values(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std \n",
        "        # predict scaled\n",
        "        y_scaled = self.session.run(self.predictions, feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        return y\n",
        "\n",
        "    def predict_values_and_derivs(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std\n",
        "        # predict scaled\n",
        "        y_scaled, dyscaled_dxscaled = self.session.run(\n",
        "            [self.predictions, self.derivs_predictions], \n",
        "            feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        dydx = self.y_std / self.x_std * dyscaled_dxscaled\n",
        "        return y, dydx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9qNr1rdEmrjT"
      },
      "outputs": [],
      "source": [
        "def test(generator, \n",
        "         sizes, \n",
        "         nTest, \n",
        "         simulSeed=None, \n",
        "         testSeed=None, \n",
        "         weightSeed=None, \n",
        "         deltidx=0):\n",
        "\n",
        "    # simulation\n",
        "    print(\"simulating training, valid and test sets\")\n",
        "    xTrain, yTrain, dydxTrain = generator.trainingSet(max(sizes), seed=simulSeed)\n",
        "    xTest, xAxis, yTest, dydxTest, vegas = generator.testSet(num=nTest, seed=testSeed)\n",
        "    print(\"done\")\n",
        "\n",
        "    # neural approximator\n",
        "    print(\"initializing neural appropximator\")\n",
        "    regressor = Neural_Approximator(xTrain, yTrain, dydxTrain)\n",
        "    print(\"done\")\n",
        "    \n",
        "    predvalues = {}    \n",
        "    preddeltas = {}\n",
        "    for size in sizes:        \n",
        "            \n",
        "        print(\"\\nsize %d\" % size)\n",
        "        regressor.prepare(size, False, weight_seed=weightSeed)\n",
        "            \n",
        "        t0 = time.time()\n",
        "        regressor.train(\"standard training\")\n",
        "        predictions, deltas = regressor.predict_values_and_derivs(xTest)\n",
        "        predvalues[(\"standard\", size)] = predictions\n",
        "        preddeltas[(\"standard\", size)] = deltas[:, deltidx]\n",
        "        t1 = time.time()\n",
        "        \n",
        "        regressor.prepare(size, True, weight_seed=weightSeed)\n",
        "            \n",
        "        t0 = time.time()\n",
        "        regressor.train(\"differential training\")\n",
        "        predictions, deltas = regressor.predict_values_and_derivs(xTest)\n",
        "        predvalues[(\"differential\", size)] = predictions\n",
        "        preddeltas[(\"differential\", size)] = deltas[:, deltidx]\n",
        "        t1 = time.time()\n",
        "        \n",
        "    return xAxis, yTest, dydxTest[:, deltidx], vegas, predvalues, preddeltas, regressor\n",
        "\n",
        "def graph(title, \n",
        "          predictions, \n",
        "          xAxis, \n",
        "          xAxisName, \n",
        "          yAxisName, \n",
        "          targets, \n",
        "          sizes, \n",
        "          computeRmse=False, \n",
        "          weights=None):\n",
        "    \n",
        "    numRows = len(sizes)\n",
        "    numCols = 2\n",
        "\n",
        "    fig, ax = plt.subplots(numRows, numCols, squeeze=False)\n",
        "    fig.set_size_inches(4 * numCols + 1.5, 4 * numRows)\n",
        "\n",
        "    for i, size in enumerate(sizes):\n",
        "        ax[i,0].annotate(\"size %d\" % size, xy=(0, 0.5), \n",
        "          xytext=(-ax[i,0].yaxis.labelpad-5, 0),\n",
        "          xycoords=ax[i,0].yaxis.label, textcoords='offset points',\n",
        "          ha='right', va='center')\n",
        "  \n",
        "    ax[0,0].set_title(\"standard\")\n",
        "    ax[0,1].set_title(\"differential\")\n",
        "    \n",
        "    for i, size in enumerate(sizes):        \n",
        "        for j, regType, in enumerate([\"standard\", \"differential\"]):\n",
        "\n",
        "            if computeRmse:\n",
        "                errors = 100 * (predictions[(regType, size)] - targets)\n",
        "                if weights is not None:\n",
        "                    errors /= weights\n",
        "                rmse = np.sqrt((errors ** 2).mean(axis=0))\n",
        "                t = \"rmse %.2f\" % rmse\n",
        "            else:\n",
        "                t = xAxisName\n",
        "                \n",
        "            ax[i,j].set_xlabel(t)            \n",
        "            ax[i,j].set_ylabel(yAxisName)\n",
        "\n",
        "            ax[i,j].plot(xAxis*100, predictions[(regType, size)]*100, 'co', \\\n",
        "                         markersize=2, markerfacecolor='white', label=\"predicted\")\n",
        "            ax[i,j].plot(xAxis*100, targets*100, 'r.', markersize=0.5, label='targets')\n",
        "\n",
        "            ax[i,j].legend(prop={'size': 8}, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.suptitle(\"% s -- %s\" % (title, yAxisName), fontsize=16)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_Uoyu-Y_mrjZ"
      },
      "outputs": [],
      "source": [
        "# generates a random correlation matrix\n",
        "def genCorrel(n):\n",
        "    randoms = np.random.uniform(low=-1., high=1., size=(2*n, n))\n",
        "    cov = randoms.T @ randoms\n",
        "    invvols = np.diag(1. / np.sqrt(np.diagonal(cov)))\n",
        "    return np.linalg.multi_dot([invvols, cov, invvols])\n",
        "\n",
        "class AutoCallBS:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 n,\n",
        "                 n_exercise,\n",
        "                 T1=1, \n",
        "                 Tm=4, \n",
        "                 K=[0.9, 0.85, 0.8, 0.7],\n",
        "                 cpn=[0.03, 0.06, 0.09, 0.12],\n",
        "                 volMult=1.5):\n",
        "        \n",
        "        self.n = n\n",
        "        self.n_exercise = n_exercise\n",
        "        self.T1 = T1\n",
        "        self.Tm = Tm\n",
        "        self.K = K\n",
        "        self.volMult = volMult\n",
        "        self.cpn = cpn\n",
        "                \n",
        "    # training set: returns S1 (mxn), AC2 (mx1) and dAC2/dS1 (mxn)\n",
        "    def trainingSet(self, m, anti=True, seed=None, bktVol=0.2):\n",
        "    \n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # spots all currently 1, without loss of generality\n",
        "        self.S0 = np.repeat(1., self.n)\n",
        "        # random correl\n",
        "        self.corr = genCorrel(self.n)\n",
        "\n",
        "        # random weights\n",
        "        self.a = np.random.uniform(low=1., high=10., size=self.n)\n",
        "        self.a /= np.sum(self.a)\n",
        "        # random vols\n",
        "        vols = np.random.uniform(low=5., high = 50., size = self.n)\n",
        "        # normalize vols for a given volatility of basket, \n",
        "        # helps with charts without loss of generality\n",
        "        avols = (self.a * vols).reshape((-1,1))\n",
        "        v = np.sqrt(np.linalg.multi_dot([avols.T, self.corr, avols]).reshape(1))\n",
        "        self.vols = vols * bktVol / v\n",
        "        self.bktVol = bktVol\n",
        "\n",
        "        # Choleski etc. for simulation\n",
        "        diagv = np.diag(self.vols)\n",
        "        self.cov = np.linalg.multi_dot([diagv, self.corr, diagv])\n",
        "        self.chol = np.linalg.cholesky(self.cov) * np.sqrt((self.Tm - self.T1)/self.n_exercise)\n",
        "        # increase vols for simulation of X so we have more samples in the wings\n",
        "        self.chol0 = self.chol * self.volMult * np.sqrt(self.T1/(self.Tm - self.T1)*self.n_exercise)\n",
        "        # simulations\n",
        "        normals = np.random.normal(size=[self.n_exercise + 1, m, self.n])\n",
        "        # R1 = np.exp(-0.5*self.bktVol**2*self.T1 + normals[0, :, :] @ self.chol0.T)\n",
        "        R1 = np.random.uniform(low=0.1, high = 1.5, size=[m, self.n])\n",
        "        S1 = self.S0 * R1\n",
        "        R_dic = {}\n",
        "        R_dic['R1'] = R1\n",
        "        S_dic = {}\n",
        "        S_dic['S1'] = S1\n",
        "        for t_i in range(self.n_exercise):\n",
        "            R_dic['R{}'.format(t_i+2)] = np.exp(-0.5*self.vols**2*(self.Tm - self.T1)/self.n_exercise + normals[t_i+1, :, :] @ self.chol.T)\n",
        "            S_dic['S{}'.format(t_i+2)] = S_dic['S{}'.format(t_i+1)] * R_dic['R{}'.format(t_i+2)]\n",
        "        \n",
        "        pay, unrefund_idx, oh_idx, oh_value = self.payoff(S_dic)\n",
        "        print('probability of path ', np.mean(unrefund_idx))\n",
        "        # two antithetic paths\n",
        "        if anti:\n",
        "            Ra_dic = {}\n",
        "            Ra_dic['R1'] = R1\n",
        "            Sa_dic = {}\n",
        "            Sa_dic['S1'] = S1\n",
        "            for t_i in range(self.n_exercise):\n",
        "                Ra_dic['R{}'.format(t_i+2)] = np.exp(-0.5*self.vols**2*(self.Tm - self.T1)/self.n_exercise - normals[t_i+1, :, :] @ self.chol.T)\n",
        "                Sa_dic['S{}'.format(t_i+2)] = Sa_dic['S{}'.format(t_i+1)] * Ra_dic['R{}'.format(t_i+2)]\n",
        "            \n",
        "            paya, unrefund_idx_a, oh_idx_a, oh_value = self.payoff(Sa_dic)\n",
        "            print('probability of anti path ', np.mean(unrefund_idx_a))\n",
        "            X = S1\n",
        "            Y = 0.5 * (pay + paya)\n",
        "\n",
        "            STm = S_dic['S{}'.format(self.n_exercise+1)]\n",
        "            STma = Sa_dic['S{}'.format(self.n_exercise+1)]\n",
        "            \n",
        "            # differentials\n",
        "            # Z1 = unrefund_idx.reshape((-1,1)) * np.min(STm, axis=1).reshape((-1,1)) / S1 # smoking_adjoints\n",
        "            # Z2 = unrefund_idx_a.reshape((-1,1)) * np.min(STma, axis=1).reshape((-1,1)) / S1\n",
        "            Z1 =  np.where(np.min(STm, axis=1).reshape((-1,1)) == STm, 1.0, 0.0) * (unrefund_idx.reshape((-1,1)) * STm / S1 + oh_idx.reshape((-1,1)) * oh_value)\n",
        "            Z2 =  np.where(np.min(STma, axis=1).reshape((-1,1)) == STma, 1.0, 0.0) * (unrefund_idx.reshape((-1,1)) * STma / S1 + oh_idx_a.reshape((-1,1)) * oh_value)\n",
        "\n",
        "            Z = 0.5 * (Z1 + Z2)\n",
        "                    \n",
        "        # standard\n",
        "        else:\n",
        "        \n",
        "            X = S1\n",
        "            Y = pay\n",
        "\n",
        "            STm = S_dic['S{}'.format(self.n_exercise+1)]\n",
        "\n",
        "            # differentials\n",
        "            Z =  np.where(np.min(STm, axis=1).reshape((-1,1)) == STm, 1.0, 0.0) * unrefund_idx.reshape((-1,1)) * STm / S1\n",
        "            # Z = unrefund_idx.reshape((-1,1)) * np.min(STm, axis=1).reshape((-1,1)) / S1 # smoking_adjoints\n",
        "        return X, Y.reshape(-1,1), Z\n",
        "\n",
        "    def payoff(self, S_dic):\n",
        "        #check validation\n",
        "        if len(S_dic) != self.n_exercise + 1:\n",
        "            print(\"path generate error\")\n",
        "            exit()\n",
        "        m = len(S_dic['S1'])\n",
        "        norefund_idx = np.ones_like(S_dic['S1'][:, 0])\n",
        "        pay = np.zeros_like(S_dic['S1'][:, 0])\n",
        "        prob = 0\n",
        "        for i, key in enumerate(S_dic):\n",
        "            if key != 'S1':\n",
        "                # print(i, self.K[i-1], key)\n",
        "                price = S_dic[key]\n",
        "                price *= norefund_idx.reshape((-1,1))\n",
        "                # refund_idx = np.all(np.maximum(0, price - self.K[i-1]*S_dic['S1']), axis=1).astype(np.float64)\n",
        "                refund_idx = np.all(np.maximum(0, price - self.K[i-1]), axis=1).astype(np.float64)\n",
        "                prob += np.mean(refund_idx)\n",
        "                pay += refund_idx * (1+self.cpn[i-1])\n",
        "                norefund_idx -= refund_idx\n",
        "        \n",
        "        unrefund_price = np.min(price * norefund_idx.reshape((-1,1)), axis=1)\n",
        "        oh_value = 5.\n",
        "        K_star = (1 + self.cpn[-1] - oh_value*self.K[-1])/(1-oh_value)\n",
        "        oh_idx = np.where(unrefund_price >= K_star, 1.0, 0.0)\n",
        "        norefund_idx -= oh_idx \n",
        "        prob += np.mean(norefund_idx + oh_idx)\n",
        "        pay += np.min(price * norefund_idx.reshape((-1,1)), axis=1)\n",
        "        pay += ((unrefund_price.reshape((-1,1))  - K_star) * oh_value + K_star) * oh_idx.reshape((-1,1))\n",
        "\n",
        "        if prob != 1.:\n",
        "            print('payoff probability is not 1', prob)\n",
        "            exit()\n",
        "        return pay, norefund_idx, oh_idx, oh_value\n",
        "\n",
        "    def pricing_AutoCall_mc(self, spots, normals):\n",
        "        \n",
        "        R_dic = {}\n",
        "        S_dic = {}\n",
        "        S_dic['S1'] = np.ones(shape=(100000, self.n)) * spots.reshape((1, -1))\n",
        "\n",
        "        for t_i in range(self.n_exercise):\n",
        "            R_dic['R{}'.format(t_i+2)] = np.exp(-0.5*self.vols**2*(self.Tm - self.T1)/self.n_exercise + normals[t_i, :, :] @ self.chol.T)\n",
        "            S_dic['S{}'.format(t_i+2)] = S_dic['S{}'.format(t_i+1)] * R_dic['R{}'.format(t_i+2)]\n",
        "        # anti\n",
        "        Ra_dic = {}\n",
        "        Sa_dic = {}\n",
        "        Sa_dic['S1'] = S_dic['S1']\n",
        "        \n",
        "        for t_i in range(self.n_exercise):\n",
        "            Ra_dic['R{}'.format(t_i+2)] = np.exp(-0.5*self.vols**2*(self.Tm - self.T1)/self.n_exercise - normals[t_i, :, :] @ self.chol.T)\n",
        "            Sa_dic['S{}'.format(t_i+2)] = Sa_dic['S{}'.format(t_i+1)] * Ra_dic['R{}'.format(t_i+2)]      \n",
        "        \n",
        "        pay, norefund_idx, oh_idx, oh_value = self.payoff(S_dic)\n",
        "        paya, norefund_idx_a, oh_idx_a, oh_value = self.payoff(Sa_dic)\n",
        "        \n",
        "        return np.mean(pay + paya) * 0.5\n",
        "        \n",
        "    # test set: returns an array of independent, uniformly random spots \n",
        "    # with corresponding baskets, ground true prices, deltas and vegas\n",
        "    def testSet(self, lower=0.5, upper=1.50, num=4096, seed=None):\n",
        "        \n",
        "        np.random.seed(seed)\n",
        "        # adjust lower and upper for dimension\n",
        "        adj = 1 + 0.5 * np.sqrt((self.n-1)*(upper-lower)/12)\n",
        "        adj_lower = 1.0 - (1.0-lower) * adj\n",
        "        adj_upper = 1.0 + (upper - 1.0) * adj\n",
        "        # draw spots\n",
        "        spots = np.random.uniform(low=adj_lower, high = adj_upper, size=(num, self.n))\n",
        "        normals = np.random.normal(size=[self.n_exercise, 100000, self.n])\n",
        "        # compute prices, deltas and vegas\n",
        "        prices = np.zeros(shape=(num, 1))\n",
        "        deltas = np.zeros(shape=(num, 1))\n",
        "        fones = np.ones(shape=(1, self.n))\n",
        "        fones[0][0] += 0.01 \n",
        "        bones = np.ones(shape=(1, self.n))\n",
        "        bones[0][0] -= 0.01\n",
        "\n",
        "        f1spots = spots * fones\n",
        "        b1spots = spots * bones\n",
        "\n",
        "        for i in range(num):\n",
        "            prices[i] += self.pricing_AutoCall_mc(spots[i], normals)\n",
        "            deltas[i] += (self.pricing_AutoCall_mc(f1spots[i], normals) - self.pricing_AutoCall_mc(b1spots[i], normals)) / (2*spots[i][0]*0.01)\n",
        "        prices = prices.reshape((-1, 1))\n",
        "        deltas = deltas.reshape((-1, 1))\n",
        "        vegas = 0\n",
        "        return spots, spots, prices, deltas, vegas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# simulation set sizes to perform\n",
        "sizes = [100000, 200000]\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "deltidx = 0 # show delta to first stock\n",
        "\n",
        "# seed\n",
        "simulSeed = 605\n",
        "# simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "testSeed = None\n",
        "weightSeed = None\n",
        "    \n",
        "# number of test scenarios\n",
        "nTest = 40    \n",
        "\n",
        "# go\n",
        "generator = AutoCallBS(3, 4)\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas, regressor = \\\n",
        "    test(generator, sizes, nTest, simulSeed, None, weightSeed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "qBFjZtSQYXaR",
        "outputId": "2312b321-7cf4-4cc9-8118-5b007fea0419"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using seed 605\n",
            "simulating training, valid and test sets\n",
            "-0.6475\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9c45648e3fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoCallBS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mxAxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdydxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvegas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulSeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightSeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-ce13d19a4c58>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(generator, sizes, nTest, simulSeed, testSeed, weightSeed, deltidx)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"simulating training, valid and test sets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdydxTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimulSeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxAxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdydxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvegas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestSeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-91c85b3b0975>\u001b[0m in \u001b[0;36mtrainingSet\u001b[0;34m(self, m, anti, seed, bktVol)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mS_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mR_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'R{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mpay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrefund_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'probability of path '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munrefund_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# two antithetic paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-91c85b3b0975>\u001b[0m in \u001b[0;36mpayoff\u001b[0;34m(self, S_dic)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0moh_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munrefund_price\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mK_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mnorefund_idx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0moh_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorefund_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moh_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (200000,) doesn't match the broadcast shape (200000,200000)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "aL-M_FDoAInr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show predicitions\n",
        "graph(\"AutoCall seed %d\" % simulSeed , values, xAxis, \"\", \"values\", yTest, sizes, True)\n",
        "\n",
        "# show deltas\n",
        "if showDeltas:\n",
        "    graph(\"AutoCall\", deltas, xAxis[:,0], \"\", \"deltas\", dydxTest, sizes, True)"
      ],
      "metadata": {
        "id": "_7bsML1gYH4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ones = np.ones((100, 3))\n",
        "\n",
        "for i in range(100):\n",
        "    ones[i][0] = (i+1)*0.01 + 0.5\n",
        "\n",
        "values_1, deltas_1 = regressor.predict_values_and_derivs(ones)\n",
        "\n",
        "normals = np.random.normal(size=[4, 100000, 3])\n",
        "\n",
        "\n",
        "targets = np.zeros_like(values_1)\n",
        "targets_delta = np.zeros_like(values_1)\n",
        "for i in range(100):\n",
        "    targets[i] += generator.pricing_AutoCall_mc(ones[i], normals)\n",
        "\n",
        "(values_1 - targets)*10000"
      ],
      "metadata": {
        "id": "PS3cDJ3jNmUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.vols"
      ],
      "metadata": {
        "id": "bN0eis2fDfJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.x_mean, regressor.x_std"
      ],
      "metadata": {
        "id": "O4LYbdR0Dy4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.x_raw"
      ],
      "metadata": {
        "id": "DJKAY0BNWudM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.y_raw"
      ],
      "metadata": {
        "id": "PGYgc7dxQA0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.dydx_raw"
      ],
      "metadata": {
        "id": "trbuvjAFRE52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdbxX3DuTOn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}